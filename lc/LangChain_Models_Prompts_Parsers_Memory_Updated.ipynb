{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amsac/ML_Notebooks/blob/main/lc/LangChain_Models_Prompts_Parsers_Memory_Updated.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **LangChain ü¶úüîó: Models, Prompts, Output Parsers and Memory**"
      ],
      "metadata": {
        "id": "2dl4DrfZNzNz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Objectives:\n",
        "\n",
        "At the end of the experiment you will be able to understand & use :\n",
        " 1. Direct API call to OpenAI\n",
        " 2. API calls through LangChain:\n",
        "   * Prompts\n",
        "   * Models\n",
        "   * Output parsers\n",
        "   * Memory"
      ],
      "metadata": {
        "id": "DxNRAJTZtvFJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xdSwQo5VL9yB",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Install the OpenAI Python client library using pip.\n",
        "# This package allows interaction with OpenAI's GPT models like GPT-3 and GPT-4.\n",
        "!pip install openai\n",
        "\n",
        "# Install the core LangChain library using pip.\n",
        "# LangChain is a framework for developing applications using language models and chaining them together.\n",
        "!pip install langchain-core\n",
        "\n",
        "# Install the LangChain Community library using pip.\n",
        "# This package includes additional components and functionality contributed by the LangChain community.\n",
        "!pip install langchain_community"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the openai library, which allows interaction with OpenAI's API to access GPT models like GPT-3 and GPT-4.\n",
        "import openai\n",
        "\n",
        "# Import the os library to interact with the operating system, such as accessing environment variables.\n",
        "import os"
      ],
      "metadata": {
        "id": "0LTu3ADENPjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Open the file containing the OpenAI API key (assuming it's stored in '/content/ts_openapi_key.txt')\n",
        "f = open('/content/ts_openapi_key.txt')\n",
        "\n",
        "# Read the contents of the file and store it as the API key.\n",
        "api_key = f.read()\n",
        "\n",
        "# Set the 'OPENAI_API_KEY' environment variable to the value of the API key.\n",
        "os.environ['OPENAI_API_KEY'] = api_key\n",
        "\n",
        "# Assign the API key to OpenAI's API configuration so it can authenticate API requests.\n",
        "openai.api_key = os.getenv('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "szWDYDp5NcXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: LLMs do not always produce the same results. When executing the code in your notebook, you may get slightly different answers than the demo."
      ],
      "metadata": {
        "id": "oZhn3CsLN3s9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Chat API : OpenAI**\n",
        "\n",
        "Let's start with a direct API calls to OpenAI."
      ],
      "metadata": {
        "id": "DmW_k527N6DI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of the OpenAI client using the OpenAI library.\n",
        "client = openai.OpenAI()\n",
        "\n",
        "# Define a function `llm_response` that takes a prompt and model name (default is 'gpt-4o-mini').\n",
        "def llm_response(prompt, model=\"gpt-4o-mini\"):\n",
        "    # Prepare the messages in a format required by the OpenAI API.\n",
        "    # The message has the role 'user' and the content as the prompt provided to the function.\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "\n",
        "    # Make a request to OpenAI's API using the client object.\n",
        "    # It uses the provided model (default is 'gpt-4o-mini') and the messages defined above.\n",
        "    # Set the temperature to 0 to make the responses deterministic.\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=0  # Ensures that responses are less random (deterministic).\n",
        "    )\n",
        "\n",
        "    # Return the content of the first response choice (the response generated by the model).\n",
        "    return response.choices[0].message.content"
      ],
      "metadata": {
        "id": "iahlnNAL5hTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_response(\"What is 1+1?\")"
      ],
      "metadata": {
        "id": "R5qi8MUHOuM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assigning a string value to the variable 'customer_email'\n",
        "# The string contains a description of an issue with a washing machine that stops after 15 minutes\n",
        "# The customer believes that the problem is due to a clogged outlet or inlet water pipe\n",
        "# The message also includes a sense of urgency, as the customer is requesting immediate help\n",
        "customer_email = \"\"\"\n",
        "My washing machine stops after 15 minutes. \\\n",
        "Something is clogged in outlet or inlet water pipe. \\\n",
        "I need your help \\\n",
        "right now, buddy!\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "EXYVhDOwRmhV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assigning a string value to the variable 'style'\n",
        "# The string specifies the desired style for communication, which is:\n",
        "# - \"bhojpuri\": The language style to be used (Bhojpuri).\n",
        "# - \"in a calm and respectful tone\": The tone in which the response should be delivered (calm and respectful).\n",
        "style = \"\"\"bhojpuri\\\n",
        "in a calm and respectful tone\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "5VDIRcjZR_dX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The 'prompt' variable is being assigned a formatted string.\n",
        "# This string is used to create a translation request with specific instructions.\n",
        "# The text should be translated into the style specified by the 'style' variable (in this case, Bhojpuri with a calm and respectful tone).\n",
        "# The 'customer_email' variable contains the original text that is to be translated.\n",
        "prompt = f\"\"\"Translate the text \\\n",
        "into a style that is {style}.\n",
        "text: ```{customer_email}```\n",
        "\"\"\"\n",
        "\n",
        "# Printing the 'prompt' to check the generated translation instructions.\n",
        "print(prompt)"
      ],
      "metadata": {
        "id": "Gnpu07qTSFdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The 'llm_response' function is called with the 'prompt' variable as the argument.\n",
        "# This sends the prompt to the language model (defined in the 'llm_response' function) to generate a response.\n",
        "response = llm_response(prompt)\n",
        "\n",
        "# The 'response' variable holds the content generated by the language model in response to the prompt.\n",
        "# We then print the response to view the model's output.\n",
        "response"
      ],
      "metadata": {
        "id": "xTiiLPST6owN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Chat API : LangChain**\n",
        "\n",
        "Let's try how we can do the same using LangChain.\n",
        "\n"
      ],
      "metadata": {
        "id": "PY1GKYRGTAeE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Model**\n",
        "https://python.langchain.com/docs/integrations/chat/openai/"
      ],
      "metadata": {
        "id": "XuzGHElGYj9-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-openai"
      ],
      "metadata": {
        "id": "f3Q2eShK4Z2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "# This is a langchain abstraction for the chatGPT API endpoint"
      ],
      "metadata": {
        "id": "G3Ah36pJ41di"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To control the randomness and creativity of the generated\n",
        "# text by an LLM, use temperature = 0.0\n",
        "llm_model=\"gpt-4o-mini\" # default : gpt-3.5-turbo\n",
        "# Initializing a ChatOpenAI instance with the specified model and temperature setting\n",
        "# The temperature parameter controls how random or deterministic the responses will be.\n",
        "# A temperature of 0.0 means deterministic responses.\n",
        "chat = ChatOpenAI(model=llm_model, temperature = 0)\n",
        "# The `chat` object is now ready to interact with the language model (gpt-4o-mini)\n",
        "# to process user inputs with a deterministic approach.\n",
        "chat"
      ],
      "metadata": {
        "id": "vMP7mAQGXRY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat.model_name"
      ],
      "metadata": {
        "id": "cBqn6yI308Gy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result=chat.invoke(\"write a poem about my red pen in 3 lines only\")\n",
        "result"
      ],
      "metadata": {
        "id": "4uF-HzQ39Gxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(result.content)"
      ],
      "metadata": {
        "id": "bz9bChUZ9Q7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(result.content)"
      ],
      "metadata": {
        "id": "il8fOIcPMbdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Prompt template**\n",
        "https://python.langchain.com/docs/concepts/#prompt-templates\n",
        "\n",
        "`PromptTemplate.from_template()`: Suitable for single-turn, non-chat prompts.\n",
        "\n",
        "`ChatPromptTemplate.from_template()`: Suitable for crafting single-turn chat prompts, typically involving chat models.\n",
        "\n",
        "`ChatPromptTemplate.from_messages()`: Suitable for multi-turn, conversational chat prompts, where the entire chat history is constructed from previous messages."
      ],
      "metadata": {
        "id": "ieZQLMeUYhXs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template_s = \"\"\"Translate the text \\\n",
        "into {style1}.\\\n",
        "text: ```{text1}```\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "rC6fRpIuYgCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now repeatedly use this template:"
      ],
      "metadata": {
        "id": "Ov-W4j37bT7v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the ChatPromptTemplate class from langchain_core.prompts\n",
        "# This class is used to create and manage templates for chat-based prompts.\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# Creating a ChatPromptTemplate instance using a template string (template_s).\n",
        "# The from_template method takes a template string and converts it into a\n",
        "# structured prompt template that can be used to format messages.\n",
        "prompt_template = ChatPromptTemplate.from_template(template_s)"
      ],
      "metadata": {
        "id": "5U_WlwjlY1wZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Accessing the first message's prompt from the ChatPromptTemplate object\n",
        "# 'messages' is a list of message objects that the prompt template holds.\n",
        "# The first message in the list is accessed using the index [0].\n",
        "# 'prompt' is the actual template string or prompt defined for that specific message.\n",
        "prompt_template.messages[0].prompt"
      ],
      "metadata": {
        "id": "tWJk1Y56Zm0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Accessing the 'input_variables' attribute from the prompt object within the first message in the ChatPromptTemplate\n",
        "# 'input_variables' is a list that contains the names of the input variables that are expected to be filled when\n",
        "# the prompt is formatted. These variables are placeholders in the template that will be replaced with actual values\n",
        "# when creating the final prompt message.\n",
        "prompt_template.messages[0].prompt.input_variables"
      ],
      "metadata": {
        "id": "SV6xoQqMZudT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining a variable 'customer_style' that specifies a tone and language style for the translation\n",
        "# The style is defined as \"Hindi\" for language and \"calm and respectful tone\" to guide the tone of the response.\n",
        "# This can be used as input for a prompt template where the desired output needs to match this style.\n",
        "customer_style = \"\"\"Hindi \\\n",
        "in a calm and respectful tone\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "A8ZzoWzgZ4VE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Customer End**"
      ],
      "metadata": {
        "id": "TLpRgcdB6xi-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining a string 'customer_email' that contains a message from a customer describing an issue with their washing machine.\n",
        "# The customer mentions that the washing machine stops after 15 minutes, speculating a possible clog in the outlet or inlet water pipe,\n",
        "# and is requesting immediate assistance in a casual tone.\n",
        "customer_email = \"\"\"\n",
        "My washing machine stops after 15 minutes. \\\n",
        "Something is clogged in outlet or inlet water pipe. \\\n",
        "I need your help \\\n",
        "right now, buddy!\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "vU8wb-z8Z-c6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the prompt template 'prompt_template' to format the input customer style and email message into a structure\n",
        "# expected by the language model. The 'format_messages' method substitutes the placeholders in the template with the provided values:\n",
        "# - 'style1' is replaced with the 'customer_style' (which specifies how the response should be styled).\n",
        "# - 'text1' is replaced with the 'customer_email' (the email message from the customer).\n",
        "# This creates a message that the language model will use to generate a response.\n",
        "customer_messages = prompt_template.format_messages(\n",
        "                    style1=customer_style,\n",
        "                    text1=customer_email)"
      ],
      "metadata": {
        "id": "o3jRvkG6aCNK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing the type of 'customer_messages' to check what kind of object it is.\n",
        "# 'customer_messages' is expected to be a list containing the formatted messages.\n",
        "print(type(customer_messages))\n",
        "\n",
        "# Printing the type of the first element in 'customer_messages'.\n",
        "# This is to check the structure of the individual message within the list.\n",
        "# Each element is expected to be an instance of a specific class (likely related to message representation in the LangChain library).\n",
        "print(type(customer_messages[0]))"
      ],
      "metadata": {
        "id": "CPTTNr0-aH9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(customer_messages[0])"
      ],
      "metadata": {
        "id": "Vze3niKuaVOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the LLM to translate to the style of the customer message\n",
        "customer_response = chat.invoke(customer_messages)"
      ],
      "metadata": {
        "id": "UZVtQigefS0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(customer_response.content)"
      ],
      "metadata": {
        "id": "Zb347qlefgyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Customer Support END**"
      ],
      "metadata": {
        "id": "Ji0EtV8N6sHv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "service_reply = \"\"\"‡§á‡§®‡§≤‡•á‡§ü ‡§î‡§∞ ‡§Ü‡§â‡§ü‡§≤‡•á‡§ü ‡§®‡§≤‡•Ä ‡§ñ‡•ã‡§≤‡•á‡§Ç\\\n",
        "‡§î‡§∞ ‡§™‡§æ‡§á‡§™ ‡§∏‡§æ‡§´ ‡§ï‡§∞‡•á‡§Ç\\\n",
        "‡§∏‡§æ‡§Æ‡§®‡•á ‡§¶‡§æ‡§π‡§ø‡§®‡•Ä ‡§ì‡§∞ ‡§®‡•Ä‡§ö‡•á ‡§è‡§ï ‡§®‡•ã‡§¨ ‡§≠‡•Ä ‡§ñ‡•ã‡§≤‡•á‡§Ç\\\n",
        "‡§â‡§∏‡•á ‡§≠‡•Ä ‡§∏‡§æ‡§´‡§º ‡§ï‡§∞‡•ã. \\\n",
        "‡§Ü‡§ó‡•á ‡§ï‡•Ä ‡§ï‡§†‡§ø‡§®‡§æ‡§à ‡§ï‡•á ‡§≤‡§ø‡§è ‡§π‡§Æ‡§∏‡•á ‡§∏‡§Ç‡§™‡§∞‡•ç‡§ï ‡§ï‡§∞‡•á‡§Ç\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "2UzyJl8SfrLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "service_style = \"\"\"English \\\n",
        "in a calm and respectful tone\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "K8HLFJlAgqar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Format the prompt template using the provided style and text to generate the service reply messages.\n",
        "# This creates a list of formatted messages based on the template, filling in placeholders with the given 'service_style' and 'service_reply' values.\n",
        "service_messages = prompt_template.format_messages(\n",
        "    style1=service_style,\n",
        "    text1=service_reply)\n",
        "\n",
        "# Print the content of the first message in the formatted 'service_messages' list.\n",
        "# This prints the actual content of the first message generated by the template, which should contain the service reply in the specified style.\n",
        "print(service_messages[0].content)"
      ],
      "metadata": {
        "id": "R-8v0I-EhhRL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Invoke the 'chat' model with the formatted service messages (from the prompt template) as input.\n",
        "# This generates a response from the language model based on the service-related information in 'service_messages'.\n",
        "service_response = chat.invoke(service_messages)\n",
        "\n",
        "# Print the content of the response generated by the language model.\n",
        "# The 'service_response.content' contains the model's reply to the service-related input.\n",
        "print(service_response.content)"
      ],
      "metadata": {
        "id": "J1Rev6_Tiecm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Output Parsers**\n",
        "\n",
        "Given below is an example of customer review:"
      ],
      "metadata": {
        "id": "Y-CyIL_Z6dOk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "customer_review_1 = \"\"\"\\\n",
        "This vacuum cleaner is absolutely fantastic. It comes with four different modes: light suction, medium breeze,\\\n",
        " strong wind, and cyclone power. It was delivered within three days, just in time for my husband's birthday surprise.\\\n",
        "  I believe he was really impressed by it. Up to now, I've been the only one operating it, and I've been using it\\\n",
        "   regularly to tidy up the floors in our home. It does cost a bit more than other vacuum cleaners on the market,\\\n",
        "    but in my opinion, it's totally worth the investment considering its extra functionalities.\n",
        "    \"\"\""
      ],
      "metadata": {
        "id": "CGpGLLxLbjoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "{\n",
        "  \"gift\": True,\n",
        "  \"delivery_days\": 2,\n",
        "  \"price_value\": \"slightly more expensive than the other leaf blowers \\\n",
        "out there, but I think it's worth it for the extra features\"\n",
        "}"
      ],
      "metadata": {
        "id": "JYjp0INI6g5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "customer_review_2 = \"\"\"\\\n",
        "This leaf blower is pretty amazing.  It has four settings:\\\n",
        "candle blower, gentle breeze, windy city, and tornado. \\\n",
        "It arrived in two days, just in time for my wife's \\\n",
        "anniversary present. \\\n",
        "I think my wife liked it so much she was speechless. \\\n",
        "So far I've been the only one using it, and I've been \\\n",
        "using it every other morning to clear the leaves on our lawn. \\\n",
        "It's slightly more expensive than the other leaf blowers \\\n",
        "out there, but I think it's worth it for the extra features.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "916R5_DYgkHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From above customer review: we want to get information as given below:"
      ],
      "metadata": {
        "id": "xG1rY5Pfglc8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "review_template = \"\"\"\\\n",
        "For the following text, extract the following information:\n",
        "\n",
        "gift: Was the item purchased as a gift or present for someone else? \\\n",
        "Answer True if yes, False if not or unknown.\n",
        "\n",
        "delivery_days: How many days did it take for the product \\\n",
        "to arrive? If this information is not found, output -1.\n",
        "\n",
        "price_value: Extract any sentences about the value or price,\\\n",
        "and output them as a comma separated Python list.\n",
        "\n",
        "Format the output as JSON with the following keys:\n",
        "gift\n",
        "delivery_days\n",
        "price_value\n",
        "\n",
        "text: {text}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "NtRhY7vv6jo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the 'ChatPromptTemplate' class from the 'langchain_core.prompts' module.\n",
        "# This class is used to create a prompt template for chat-based interactions.\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# 'from_template' is a method of the 'ChatPromptTemplate' class used to create a prompt template\n",
        "# based on the provided template string 'review_template'. This template will be used to format chat prompts.\n",
        "prompt_template = ChatPromptTemplate.from_template(review_template)\n",
        "\n",
        "# Printing the 'prompt_template' object to check its structure or content after creating it.\n",
        "# This is helpful for debugging or inspecting the prompt that has been generated.\n",
        "print(prompt_template)"
      ],
      "metadata": {
        "id": "dNnDQGzR6m5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the 'format_messages' method of the 'prompt_template' to format the 'customer_review_1' text\n",
        "# into the structure required by the prompt template. This will generate a list of messages that the model can understand.\n",
        "messages = prompt_template.format_messages(text=customer_review_1)\n",
        "\n",
        "# Creating an instance of the 'ChatOpenAI' class from the LangChain library.\n",
        "# The 'temperature' is set to 0.0, which means the model will generate deterministic responses (low randomness).\n",
        "# The 'temperature' controls how creative the model's responses are.\n",
        "chat = ChatOpenAI(temperature=0.0)\n",
        "\n",
        "# Invoking the 'chat' model with the formatted 'messages'.\n",
        "# This will send the formatted messages to the model and receive a response.\n",
        "response = chat.invoke(messages)\n",
        "\n",
        "# Printing the content of the response from the model.\n",
        "# The 'content' attribute holds the actual response text generated by the model.\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "5pOTyxKn6pyl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(response.content)"
      ],
      "metadata": {
        "id": "moHN5KrA6r-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# You will get an error by running this line of code\n",
        "# because'gift' is not a dictionary\n",
        "# 'gift' is a string\n",
        "response.content.get('gift')"
      ],
      "metadata": {
        "id": "yvos0gSi6t-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Parse the LLM output string into a structured data**::\n",
        "\n"
      ],
      "metadata": {
        "id": "Pf3bgzsf7pgP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [Structured Output](https://python.langchain.com/docs/how_to/structured_output/)"
      ],
      "metadata": {
        "id": "d8N7zx3mCoIj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing BaseModel and Field from the pydantic library\n",
        "# Pydantic is a library used for data validation and settings management using Python type annotations.\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "# Defining a Pydantic model called 'product_info' that inherits from BaseModel.\n",
        "# This model will be used to structure and validate the input data for a product's service info.\n",
        "class product_info(BaseModel):\n",
        "    \"\"\" Product service info.\"\"\"\n",
        "\n",
        "    # A field 'gift' to determine whether the product was purchased as a gift.\n",
        "    # 'Field' allows us to add descriptions and constraints to the field.\n",
        "    # 'str' indicates that this field will store string data, and the description is added for clarity.\n",
        "    gift: str = Field(description=\"Was the item purchased as a gift for someone else? \\\n",
        "                                  Answer True if yes, False if not or unknown.\")\n",
        "\n",
        "    # A field 'delivery_days' to store the number of days it took for the product to arrive.\n",
        "    # If the data is unavailable, it should output -1. 'int' indicates that this field will store integer data.\n",
        "    delivery_days: int = Field(description=\"How many days did it take for the product\\\n",
        "                                           to arrive? If this information is not found,\\\n",
        "                                           output -1.\")\n",
        "\n",
        "    # A field 'price_value' that stores sentences about the value or price of the product.\n",
        "    # The description explains that sentences should be extracted and output as a comma-separated list.\n",
        "    price_value: str = Field(description=\"Extract any sentences about the value or\\\n",
        "                                         price, and output them as a comma separated Python list.\"\n",
        "    )"
      ],
      "metadata": {
        "id": "gMNEo-laCqTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the 'with_structured_output' method to set up structured output for the LLM\n",
        "# This means that the LLM will return results in a format that conforms to the 'product_info' Pydantic model.\n",
        "structured_llm = chat.with_structured_output(produt_info)\n",
        "\n",
        "# Invoking the structured LLM to process the input data (`customer_review_2`).\n",
        "# The input (`customer_review_2`) should contain information about the product that will be validated\n",
        "# and returned as an instance of the 'product_info' model.\n",
        "result = structured_llm.invoke(customer_review_2)\n",
        "\n",
        "# The 'result' now contains the structured output, which can be further used or printed.\n",
        "result"
      ],
      "metadata": {
        "id": "yzT5yJYZHlvN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(result.gift)\n",
        "print(result.delivery_days)\n",
        "print(result.price_value)"
      ],
      "metadata": {
        "id": "-wR8Y_JQC51h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **[More detail and pydantic output parser](https://python.langchain.com/docs/how_to/output_parser_structured/)**\n",
        "\n",
        "* Note: The [old version]((https://python.langchain.com/v0.1/docs/modules/model_io/output_parsers/types/structured/)) had different methods for output parsers:\n",
        "\n",
        "  `from langchain.output_parsers import ResponseSchema, StructuredOutputParser`"
      ],
      "metadata": {
        "id": "OaAYLfqRF_yl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **LangChain: Memory**\n",
        "#### [**Customizing Conversational Memory**](https://python.langchain.com/docs/how_to/chatbots_memory/)\n",
        "\n",
        "LangChain can helps in building better chatbots, or have\n",
        "an LLM with more effective chats by better managing\n",
        "what it remembers from the conversation you've had so far.\n"
      ],
      "metadata": {
        "id": "V5Ftas7aMWAj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing ChatMessageHistory from langchain_community\n",
        "# This class is used to store and manage the history of chat messages in a conversation.\n",
        "from langchain_community.chat_message_histories import ChatMessageHistory\n",
        "\n",
        "# Importing RunnableWithMessageHistory from langchain_core.runnables.history\n",
        "# This class allows you to run a specific chain or function while maintaining a history of the conversation's messages.\n",
        "# It is useful when you want to preserve the context of a conversation for use in subsequent interactions.\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory"
      ],
      "metadata": {
        "id": "z2a4kdKTMWvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a ChatPromptTemplate using from_messages method\n",
        "# This method constructs a prompt template from a list of messages\n",
        "# where the system, user, and placeholder messages are defined.\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        # The system message defines the assistant's role and behavior.\n",
        "        # It helps set the context for how the assistant should answer questions.\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are a helpful assistant. Answer all questions to the best of your ability.\",\n",
        "        ),\n",
        "        # Placeholder for chat history; this is where previous interactions would be injected.\n",
        "        (\"placeholder\", \"{chat_history}\"),\n",
        "        # The human message is the user query; this is where the user's input would be placed.\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Chain the prompt with the chat model to create a process where\n",
        "# the prompt template is used along with the chat model to generate responses.\n",
        "# LCEL refers to LangChain Expression Language, which helps in defining how different parts (like prompt and model) should interact.\n",
        "chain = prompt | chat # LCEL"
      ],
      "metadata": {
        "id": "wc_PduTSMA5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the RunnableWithMessageHistory class from langchain_core.runnables.history\n",
        "# This class allows attaching message history functionality to a runnable chain.\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "\n",
        "# Create an instance of ChatMessageHistory to store and retrieve message history.\n",
        "# ChatMessageHistory keeps track of the messages exchanged during the conversation.\n",
        "chat_history = ChatMessageHistory()\n",
        "\n",
        "# Create a chain that includes message history functionality.\n",
        "# The chain is composed of:\n",
        "# - 'chain': the base chain (which generates responses)\n",
        "# - A lambda function that links each session to the `chat_history` instance.\n",
        "#   The lambda takes a session ID (e.g., 'session_id') and returns the corresponding history.\n",
        "# - 'input_messages_key' defines the key used to extract input messages from the data.\n",
        "# - 'history_messages_key' defines the key where the chat history will be stored.\n",
        "# This allows tracking of conversation history during multiple interactions.\n",
        "chain_with_message_history = RunnableWithMessageHistory(\n",
        "    chain,  # The base chain that will generate the responses\n",
        "    lambda session_id: chat_history,  # Lambda function to link session ID to the chat history\n",
        "    input_messages_key=\"input\",  # Key for input messages\n",
        "    history_messages_key=\"chat_history\",  # Key for storing chat history\n",
        ")"
      ],
      "metadata": {
        "id": "gV2TLQC1Mmzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Invoke the chain_with_message_history with an input message and a session ID.\n",
        "# The `invoke` method processes the input message and keeps track of the session's history.\n",
        "# It takes two arguments:\n",
        "# 1. The first argument is the message input provided by the user, which is `{\"input\": \"Hi my name is Ramendra! what is 1+1?\"}`.\n",
        "#    This message contains the user's question or request, which in this case is a greeting followed by a simple math question.\n",
        "# 2. The second argument is the configuration for the session. In this case, it's a dictionary `{\"configurable\": {\"session_id\": \"user1\"}}`,\n",
        "#    which specifies that the session is associated with the user ID \"user1\".\n",
        "#    This session ID helps to store and retrieve chat history for that particular user.\n",
        "\n",
        "result = chain_with_message_history.invoke(\n",
        "    {\"input\": \"Hi my name is Ramendra! what is 1+1?\"},  # User input message\n",
        "    {\"configurable\": {\"session_id\": \"user1\"}},  # Configurable parameters, including the session ID\n",
        ")\n",
        "\n",
        "# The result stores the output generated by the chain after processing the input and message history.\n",
        "result  # This will hold the assistant's response, which would include a reply to the greeting and math question."
      ],
      "metadata": {
        "id": "PLvj2ptyNL9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result.content"
      ],
      "metadata": {
        "id": "m75g86jPNzn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Invoke the `chain_with_message_history` to process a new input message and track the chat history.\n",
        "# This time, the input message is \"Do you remember my name?\", where the user is asking if the assistant recalls their name.\n",
        "# The session ID remains \"user1\", ensuring the history of the chat is tracked for this specific user.\n",
        "\n",
        "result = chain_with_message_history.invoke(\n",
        "    input = {\"input\": \"Do you remember my name?\"},  # User asks if the assistant remembers their name.\n",
        "    config = {\"configurable\": {\"session_id\": \"user1\"}}  # Session ID is \"user1\", indicating the history for this user.\n",
        ")\n",
        "\n",
        "# The result holds the assistant's response, which includes the answer to the user's query.\n",
        "print(result)  # Print the entire result object to view the response.\n",
        "print(result.content)  # Print only the content of the response, which is likely the assistant's answer."
      ],
      "metadata": {
        "id": "qHQ7UKTkOBBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history.messages"
      ],
      "metadata": {
        "id": "dSHg4Q8nRUKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Note: The old version had different ways for memory management:**\n",
        "\n",
        "\n",
        "\n",
        "* [ConversationBufferMemory](https://python.langchain.com/v0.1/docs/modules/memory/types/buffer/): This memory allows for storing messages and then extracts the messages in a variable.\n",
        "\n",
        "* [ConversationBufferWindowMemory](https://python.langchain.com/v0.1/docs/modules/memory/types/buffer_window/):  It only uses the last K interactions.\n",
        "* [ConversationTokenBufferMemory](https://python.langchain.com/v0.1/docs/modules/memory/types/token_buffer/): It uses token length rather than number of interactions to determine when to flush interactions.\n",
        "* [ConversationSummaryMemory](https://python.langchain.com/v0.1/docs/modules/memory/types/summary/) : This type of memory creates a summary of the conversation over time.\n"
      ],
      "metadata": {
        "id": "evNdTh0U-auh"
      }
    }
  ]
}