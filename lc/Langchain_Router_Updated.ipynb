{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amsac/ML_Notebooks/blob/main/lc/Langchain_Router_Updated.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **LangChain ðŸ¦œðŸ”—:  Routing**"
      ],
      "metadata": {
        "id": "MjhfGDRHt_in"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **[Reference](https://python.langchain.com/docs/how_to/routing/)**"
      ],
      "metadata": {
        "id": "3VTu12UyxkhN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ow0D5JMrd_Ur"
      },
      "outputs": [],
      "source": [
        "# Install the OpenAI package, which allows access to OpenAI models (e.g., GPT-3, GPT-4)\n",
        "!pip install openai\n",
        "\n",
        "# Install LangChain, a framework for building applications using LLMs (Large Language Models)\n",
        "!pip install langchain\n",
        "\n",
        "# Install the LangChain OpenAI integration package to connect LangChain with OpenAI models\n",
        "!pip install langchain-openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the OpenAI Python client library, which allows us to interact with OpenAI models\n",
        "import openai\n",
        "\n",
        "# Import the OS module to interact with the operating system, like managing environment variables\n",
        "import os"
      ],
      "metadata": {
        "id": "ONj0t8WTfsBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Open the file containing the OpenAI API key\n",
        "f = open('/content/ts_openapi_key.txt')\n",
        "\n",
        "# Read the API key from the file and store it in the 'api_key' variable\n",
        "api_key = f.read()\n",
        "\n",
        "# Set the API key as an environment variable, so OpenAI can access it for authentication\n",
        "os.environ['OPENAI_API_KEY'] = api_key\n",
        "\n",
        "# Assign the environment variable value to OpenAI's API key attribute\n",
        "openai.api_key = os.getenv('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "eK9KcnIbfuCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the ChatOpenAI class from langchain_openai, which provides a wrapper for interacting with OpenAI's chat models\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Import the PromptTemplate class from langchain_core.prompts, which is used to create prompt templates\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "# Import the ChatPromptTemplate class, which helps in creating chat-based prompt templates\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# Import the StrOutputParser from langchain_core.output_parsers, which is used to parse outputs as strings\n",
        "from langchain_core.output_parsers import StrOutputParser"
      ],
      "metadata": {
        "id": "-AYTyQDpQxXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the model name for the chat model you want to use\n",
        "llm_model = \"gpt-4o-mini\"  # This is a chat model\n",
        "\n",
        "# Initialize the ChatOpenAI model with specific parameters\n",
        "llm = ChatOpenAI(temperature=0, model=llm_model)"
      ],
      "metadata": {
        "id": "HQ4GJ5CPQt3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a PromptTemplate to classify user questions into categories\n",
        "prompt_classify = PromptTemplate.from_template(\"\"\"Given the user question below, classify it as either being about\n",
        " `Physics`, `Math`, `Computer Science`. Do not respond with more than one word.\n",
        " <question> {question} </question> Classification:\"\"\")"
      ],
      "metadata": {
        "id": "w4HVAufXR_TV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_classify"
      ],
      "metadata": {
        "id": "HntYnRVefdA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a chain that combines the PromptTemplate, language model, and output parser\n",
        "chain = prompt_classify | llm | StrOutputParser()\n",
        "\n",
        "# Invoke the chain with a sample question about the first law of motion\n",
        "chain.invoke({'question': \"what is first law of motion?\"})"
      ],
      "metadata": {
        "collapsed": true,
        "id": "p_gSga7TS0ob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke({\"question\":\"What's a programming language?\"})"
      ],
      "metadata": {
        "collapsed": true,
        "id": "wJvqe-T7tkYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Template for physics-related questions and responses\n",
        "physics_template = \"\"\"You are a very smart physics professor. \\\n",
        "You are great at answering questions about physics in a concise\\\n",
        "and easy to understand manner. \\\n",
        "When you don't know the answer to a question you admit\\\n",
        "that you don't know.\n",
        "\n",
        "Here is a question:\n",
        "{question}\n",
        "Start your answer with `Subject:Physics :`\n",
        "\"\"\"\n",
        "\n",
        "# Create a ChatPromptTemplate using the physics template\n",
        "physics_prompt = ChatPromptTemplate.from_template(physics_template)\n",
        "\n",
        "# Chain the template, language model, and output parser\n",
        "physics_chain = physics_prompt | llm | StrOutputParser()"
      ],
      "metadata": {
        "id": "PSHQ2YXGo9S5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Template for math-related questions and responses\n",
        "math_template = \"\"\"You are a very good mathematician. \\\n",
        "You are great at answering math questions. \\\n",
        "You are so good because you are able to break down \\\n",
        "hard problems into their component parts,\n",
        "answer the component parts, and then put them together\\\n",
        "to answer the broader question.\n",
        "\n",
        "Here is a question:\n",
        "{question}\n",
        "Start your answer with `Subject:Math :`\n",
        "\"\"\"\n",
        "\n",
        "# Create a ChatPromptTemplate using the math template\n",
        "math_prompt = ChatPromptTemplate.from_template(math_template)\n",
        "\n",
        "# Chain the template, language model, and output parser\n",
        "math_chain = math_prompt | llm | StrOutputParser()"
      ],
      "metadata": {
        "id": "G5NIcMcFprGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Template for computer science-related questions and responses\n",
        "computerscience_template = \"\"\" You are a successful computer scientist.\\\n",
        "You have a passion for creativity, collaboration,\\\n",
        "forward-thinking, confidence, strong problem-solving capabilities,\\\n",
        "understanding of theories and algorithms, and excellent communication \\\n",
        "skills. You are great at answering coding questions. \\\n",
        "You are so good because you know how to solve a problem by \\\n",
        "describing the solution in imperative steps \\\n",
        "that a machine can easily interpret and you know how to \\\n",
        "choose a solution that has a good balance between \\\n",
        "time complexity and space complexity.\n",
        "\n",
        "Here is a question:\n",
        "{question}\n",
        "Start your answer with `Subject:Computer Science :`\n",
        "\"\"\"\n",
        "\n",
        "# Create a ChatPromptTemplate using the computer science template\n",
        "computer_science_prompt = ChatPromptTemplate.from_template(computerscience_template)\n",
        "\n",
        "# Chain the template, language model, and output parser\n",
        "computer_science_chain = computer_science_prompt | llm | StrOutputParser()"
      ],
      "metadata": {
        "id": "86AoFDSTup-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Template for general-purpose questions and responses\n",
        "general_prompt = ChatPromptTemplate.from_template(\n",
        "    \"\"\"Respond to the following question:\n",
        "\n",
        "Question: {question}\n",
        "Start your answer with `Subject:General :`\"\"\"\n",
        ")\n",
        "\n",
        "# Chain the template, language model, and output parser\n",
        "general_chain = general_prompt | llm | StrOutputParser()"
      ],
      "metadata": {
        "id": "7hTH8iE1p1Jx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def route(info):\n",
        "    # Check if the word 'physics' is in the topic, case-insensitive.\n",
        "    if \"physics\" in info[\"topic\"].lower():\n",
        "        return physics_chain  # Return the physics-specific chain if topic is related to physics.\n",
        "\n",
        "    # Check if the word 'math' is in the topic, case-insensitive.\n",
        "    elif \"math\" in info[\"topic\"].lower():\n",
        "        return math_chain  # Return the math-specific chain if topic is related to math.\n",
        "\n",
        "    # Check if the phrase 'computer science' is in the topic, case-insensitive.\n",
        "    elif \"computer science\" in info[\"topic\"].lower():\n",
        "        return computer_science_chain  # Return the computer science-specific chain if topic is related to computer science.\n",
        "\n",
        "    # If none of the above topics are found, return the general-purpose chain.\n",
        "    else:\n",
        "        return general_chain  # Return the general-purpose chain for any other topics."
      ],
      "metadata": {
        "id": "fh1nAHozq0IX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "# Define a dictionary that contains the \"topic\" and \"question\".\n",
        "# The \"topic\" maps to the appropriate chain, while the \"question\" is a lambda function\n",
        "# that extracts the \"question\" from the input data.\n",
        "full_chain = {\"topic\": chain, \"question\": lambda x: x[\"question\"]} | RunnableLambda(route)\n",
        "\n",
        "# Display the full chain pipeline\n",
        "full_chain"
      ],
      "metadata": {
        "id": "RaZF5wa0rXos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_chain.invoke({\"question\":\"What is black body radiation?\"})"
      ],
      "metadata": {
        "id": "gFqN8GDmrb88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_chain.invoke({\"question\":\"What's a programming language?\"})"
      ],
      "metadata": {
        "id": "E4mGH__8r5M3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_chain.invoke({\"question\":\"What's 1+1\"})"
      ],
      "metadata": {
        "id": "jsSERmUHsn5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_chain.invoke({\"question\":\"What's a path integral?\"}) #"
      ],
      "metadata": {
        "id": "RPn5Cznus4L0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_chain.invoke({\"question\":\"tell me 2 facts about Taj Mahal India?\"})"
      ],
      "metadata": {
        "id": "U6qTbZRCvo54"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}