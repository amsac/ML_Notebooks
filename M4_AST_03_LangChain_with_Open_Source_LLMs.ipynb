{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amsac/ML_Notebooks/blob/main/M4_AST_03_LangChain_with_Open_Source_LLMs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMNv0S7qBl1Z"
      },
      "source": [
        "# Advanced Certification Programme in AI and MLOps\n",
        "## A Program by IISc and TalentSprint\n",
        "### Assignment 3: Open Source LLMs with LangChain ðŸ¦œðŸ”—"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7nOHNxJqC2X"
      },
      "source": [
        "## Learning Objectives\n",
        "\n",
        "At the end of the experiment, you will be able to:\n",
        "\n",
        "* use open source LLMs: **`zephyr-7b-beta`**, **`Mistral-7B-Instruct-v0.2`**,  and **`Llama2`** through HuggingFaceHub with LangChain\n",
        "* understand & use the concept of Prompt template, Memory and output parsers in LangChain\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNLA8HiKxQhc"
      },
      "source": [
        "### Setup Steps:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2YzfoPvJDiTX"
      },
      "outputs": [],
      "source": [
        "#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n",
        "Id = \"\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AjoZJWGErxGf"
      },
      "outputs": [],
      "source": [
        "#@title Please enter your password (your registered phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n",
        "password = \"\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "WBPPuGmBlDIN"
      },
      "outputs": [],
      "source": [
        "#@title Run this cell to complete the setup for this Notebook\n",
        "from IPython import get_ipython\n",
        "\n",
        "ipython = get_ipython()\n",
        "\n",
        "notebook= \"M4_AST_03_LangChain_with_Open_Source_LLMs\" #name of the notebook\n",
        "\n",
        "def setup():\n",
        "#  ipython.magic(\"sx pip3 install torch\")\n",
        "\n",
        "    # ipython.magic(\"wget https://cdn.iisc.talentsprint.com/AIandMLOps/Datasets/Acoustic_Extinguisher_Fire_Dataset.xlsx\")\n",
        "    from IPython.display import HTML, display\n",
        "    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n",
        "    print(\"Setup completed successfully\")\n",
        "    return\n",
        "\n",
        "def submit_notebook():\n",
        "    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n",
        "\n",
        "    import requests, json, base64, datetime\n",
        "\n",
        "    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n",
        "    if not submission_id:\n",
        "      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "\n",
        "      if r[\"status\"] == \"Success\":\n",
        "          return r[\"record_id\"]\n",
        "      elif \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print (\"Something is wrong, the notebook will not be submitted for grading\")\n",
        "        return None\n",
        "\n",
        "    elif getAnswer() and getComplexity() and getAdditional() and getConcepts() and getComments() and getMentorSupport():\n",
        "      f = open(notebook + \".ipynb\", \"rb\")\n",
        "      file_hash = base64.b64encode(f.read())\n",
        "\n",
        "      data = {\"complexity\" : Complexity, \"additional\" :Additional,\n",
        "              \"concepts\" : Concepts, \"record_id\" : submission_id,\n",
        "              \"answer\" : Answer, \"id\" : Id, \"file_hash\" : file_hash,\n",
        "              \"notebook\" : notebook,\n",
        "              \"feedback_experiments_input\" : Comments,\n",
        "              \"feedback_mentor_support\": Mentor_support}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "      if \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print(\"Your submission is successful.\")\n",
        "        print(\"Ref Id:\", submission_id)\n",
        "        print(\"Date of submission: \", r[\"date\"])\n",
        "        print(\"Time of submission: \", r[\"time\"])\n",
        "        print(\"View your submissions: https://aimlops-iisc.talentsprint.com/notebook_submissions\")\n",
        "        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n",
        "        return submission_id\n",
        "    else: submission_id\n",
        "\n",
        "\n",
        "def getAdditional():\n",
        "  try:\n",
        "    if not Additional:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Additional\n",
        "  except NameError:\n",
        "    print (\"Please answer Additional Question\")\n",
        "    return None\n",
        "\n",
        "def getComplexity():\n",
        "  try:\n",
        "    if not Complexity:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Complexity\n",
        "  except NameError:\n",
        "    print (\"Please answer Complexity Question\")\n",
        "    return None\n",
        "\n",
        "def getConcepts():\n",
        "  try:\n",
        "    if not Concepts:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Concepts\n",
        "  except NameError:\n",
        "    print (\"Please answer Concepts Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "# def getWalkthrough():\n",
        "#   try:\n",
        "#     if not Walkthrough:\n",
        "#       raise NameError\n",
        "#     else:\n",
        "#       return Walkthrough\n",
        "#   except NameError:\n",
        "#     print (\"Please answer Walkthrough Question\")\n",
        "#     return None\n",
        "\n",
        "def getComments():\n",
        "  try:\n",
        "    if not Comments:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Comments\n",
        "  except NameError:\n",
        "    print (\"Please answer Comments Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getMentorSupport():\n",
        "  try:\n",
        "    if not Mentor_support:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Mentor_support\n",
        "  except NameError:\n",
        "    print (\"Please answer Mentor support Question\")\n",
        "    return None\n",
        "\n",
        "def getAnswer():\n",
        "  try:\n",
        "    if not Answer:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Answer\n",
        "  except NameError:\n",
        "    print (\"Please answer Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getId():\n",
        "  try:\n",
        "    return Id if Id else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "def getPassword():\n",
        "  try:\n",
        "    return password if password else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "submission_id = None\n",
        "### Setup\n",
        "if getPassword() and getId():\n",
        "  submission_id = submit_notebook()\n",
        "  if submission_id:\n",
        "    setup()\n",
        "else:\n",
        "  print (\"Please complete Id and Password cells before running setup\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WeXRnNMfnFqF"
      },
      "source": [
        "### Install required dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKLiNSv6wTr6"
      },
      "outputs": [],
      "source": [
        "# Langchain\n",
        "!pip -q install langchain\n",
        "\n",
        "# Library to communicate with HF hub\n",
        "!pip -q install --upgrade huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UAsnCf4Z9e1K"
      },
      "outputs": [],
      "source": [
        "!pip -q install langchain_community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BrFlLJv3-_4N"
      },
      "outputs": [],
      "source": [
        "!pip -q install langchain_huggingface"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0sgU1wiq7Hw"
      },
      "source": [
        "### Import required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0IydGx_q78h"
      },
      "outputs": [],
      "source": [
        "# Import the 'os' module to interact with the operating system, such as setting environment variables.\n",
        "import os\n",
        "\n",
        "# Import 'getpass' to securely prompt the user for a password or API key without displaying it on the screen.\n",
        "from getpass import getpass\n",
        "\n",
        "# Import 'HuggingFaceEndpoint' from LangChain's community module to connect and interact with Hugging Face's hosted models.\n",
        "from langchain_community.llms import HuggingFaceEndpoint\n",
        "\n",
        "# Import 'PromptTemplate' from LangChain to create structured templates for prompting language models.\n",
        "from langchain.prompts import PromptTemplate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMBDqFT0kkgH"
      },
      "source": [
        "### **Provide your HuggingFace api key/access token**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "i9TSeu4cq1Jw"
      },
      "outputs": [],
      "source": [
        "# Securely prompt the user to enter their Hugging Face access token without displaying it on the screen.\n",
        "pass_token = getpass(\"Enter your HuggingFace access token: \")\n",
        "\n",
        "# Store the entered token as an environment variable named 'HF_TOKEN'.\n",
        "os.environ[\"HF_TOKEN\"] = pass_token\n",
        "\n",
        "# Also store the token in 'HUGGINGFACEHUB_API_TOKEN', which may be required for authentication with Hugging Face's API.\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = pass_token\n",
        "\n",
        "# Delete the 'pass_token' variable from memory for security reasons after storing it in environment variables.\n",
        "del pass_token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgzDSiux-EuO"
      },
      "source": [
        "### **Exploring Open Source LLMs hosted on HuggingFace**\n",
        "\n",
        ">**I.** `HuggingFaceH4/zephyr-7b-beta`\n",
        ">\n",
        ">**II.** `mistralai/Mistral-7B-Instruct-v0.2`\n",
        ">\n",
        ">**III.** `LlaMa2`\n",
        "\n",
        "[LangChain link](https://python.langchain.com/docs/integrations/chat/huggingface) for using Hugging Face LLM's as chat models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVK3LSt6mzKa"
      },
      "source": [
        "### **I.** [**HuggingFaceH4/zephyr-7b-beta**](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "QudSou2y-T4i"
      },
      "outputs": [],
      "source": [
        "# Import HuggingFace model abstraction class from langchain\n",
        "from langchain_huggingface import HuggingFaceEndpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "jVRB1nztyTUm"
      },
      "outputs": [],
      "source": [
        "# Initialize the Hugging Face language model endpoint with specific parameters.\n",
        "llm = HuggingFaceEndpoint(\n",
        "    # Specify the repository ID of the model to use. In this case, it's \"zephyr-7b-beta\" from HuggingFaceH4.\n",
        "    repo_id=\"HuggingFaceH4/zephyr-7b-beta\",\n",
        "\n",
        "    # Define the task type for the model, which is \"text-generation\" (used for generating text responses).\n",
        "    task=\"text-generation\",\n",
        "\n",
        "    # Set the maximum number of new tokens the model can generate in response.\n",
        "    max_new_tokens=512,\n",
        "\n",
        "    # Use 'top_k' sampling to limit the number of highest-probability tokens considered at each generation step.\n",
        "    top_k=30,\n",
        "\n",
        "    # Set the temperature, which controls the randomness of the output. A lower value (e.g., 0.1) makes responses more deterministic.\n",
        "    temperature=0.1,\n",
        "\n",
        "    # Apply a repetition penalty to discourage the model from repeating the same words or phrases too frequently.\n",
        "    repetition_penalty=1.03,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "qrneNiybIFl_"
      },
      "outputs": [],
      "source": [
        "# Invoke the Hugging Face language model with a specific prompt asking for five points on learning programming.\n",
        "response = llm.invoke(\"How to learn programming? give 5 points\")\n",
        "\n",
        "# Print the model's generated response to the console.\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4N14UjkBaws"
      },
      "source": [
        "#### **Prompt Template**\n",
        "\n",
        "Prompt templates are predefined recipes for generating prompts for language models.\n",
        "\n",
        "A template may include instructions, few-shot examples, and specific context and questions appropriate for a given task.\n",
        "\n",
        "LangChain provides tooling to create and work with prompt templates.\n",
        "\n",
        "To know more about Prompt template, refer [here](https://python.langchain.com/docs/modules/model_io/prompts/quick_start)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-v-l3yx9hQs2"
      },
      "source": [
        "#### **Example-1**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "NIjgIglFbjDG"
      },
      "outputs": [],
      "source": [
        "# Import the PromptTemplate class from LangChain to create structured prompts.\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Create a prompt template using placeholders for dynamic input values.\n",
        "prompt_template = PromptTemplate.from_template(\n",
        "    \"Tell me a {adjective} joke about {content}.\"\n",
        ")\n",
        "\n",
        "# Format the template by replacing the placeholders with actual values.\n",
        "messages = prompt_template.format(adjective=\"funny\", content=\"Trump\")\n",
        "\n",
        "# Output the formatted prompt string.\n",
        "messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "iUOnAu5c_Hqw"
      },
      "outputs": [],
      "source": [
        "# Import the ChatHuggingFace class from LangChain's Hugging Face integration.\n",
        "from langchain_huggingface import ChatHuggingFace\n",
        "\n",
        "# Initialize a chat model using the previously defined Hugging Face LLM.\n",
        "chat_model = ChatHuggingFace(llm=llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "DjM9t0INbjLg"
      },
      "outputs": [],
      "source": [
        "# Invoke the chat model with the formatted prompt message.\n",
        "response = chat_model.invoke(messages)\n",
        "\n",
        "# Print the generated response content from the chat model.\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rbI1OD_ds3z"
      },
      "source": [
        "**Practice-1 :**\n",
        "Create a prompt template envisioning a situation where you have to pass three parameters, and the language model responds to it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ek31fbUK1NqT"
      },
      "outputs": [],
      "source": [
        "# YOUR CODES HERE for above practice exercise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPeo4FQBhL0l"
      },
      "source": [
        "#### **Example-2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bat0JTd7GrJx"
      },
      "outputs": [],
      "source": [
        "# Import message classes from LangChain's schema module.\n",
        "from langchain.schema import (\n",
        "    HumanMessage,  # Represents a message sent by the human (user) in a conversation.\n",
        "    SystemMessage,  # Represents a message providing system-level instructions or context.\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "mQe8qxghewsF"
      },
      "outputs": [],
      "source": [
        "# Import the PromptTemplate class from LangChain to create structured prompts.\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Create a prompt template with placeholders for dynamic values.\n",
        "prompt_template = PromptTemplate.from_template(\n",
        "    \"Tell me {count} facts about {event_or_place}.\"\n",
        ")\n",
        "\n",
        "# Format the template by replacing placeholders with actual values.\n",
        "user_msg = prompt_template.format(count=5, event_or_place=\"Tajmahal\")\n",
        "\n",
        "# Output the formatted prompt string.\n",
        "user_msg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "GdGDCoQhmuDL"
      },
      "outputs": [],
      "source": [
        "# Create a list of messages to structure a conversation with both system and human roles.\n",
        "messages = [\n",
        "    # SystemMessage provides a directive to the model about its role or behavior.\n",
        "    SystemMessage(content=\"You're a knowledgeable historian\"),\n",
        "\n",
        "    # HumanMessage contains the prompt or query from the user, which is the formatted message.\n",
        "    HumanMessage(content=user_msg),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vCpfJTPCAqGP"
      },
      "outputs": [],
      "source": [
        "# Import the ChatHuggingFace class from LangChain's Hugging Face integration to handle chat interactions.\n",
        "from langchain_huggingface import ChatHuggingFace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9kg1GDWsCRbL"
      },
      "outputs": [],
      "source": [
        "# Initialize the ChatHuggingFace model with the previously defined Hugging Face language model (llm).\n",
        "chat_model = ChatHuggingFace(llm=llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "27Iyhp-5DJ5-"
      },
      "outputs": [],
      "source": [
        "# Access the model identifier of the Hugging Face model used by the ChatHuggingFace instance.\n",
        "chat_model.model_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9niBkmKIDMVI"
      },
      "outputs": [],
      "source": [
        "# Convert the sequence of messages into a chat prompt that can be processed by the Hugging Face model.\n",
        "chat_model._to_chat_prompt(messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "px6WPyxN2DYj"
      },
      "outputs": [],
      "source": [
        "# Convert the sequence of messages into a chat prompt and print the resulting formatted prompt.\n",
        "print(chat_model._to_chat_prompt(messages))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ZduaUIkl-VpI"
      },
      "outputs": [],
      "source": [
        "# Invoke the chat model with the sequence of messages, sending it to the Hugging Face model for processing.\n",
        "response = chat_model.invoke(messages)\n",
        "\n",
        "# Print the generated response content from the chat model.\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-gtbS92fRq_"
      },
      "source": [
        "**Practice-2 :**\n",
        "Create a prompt template envisioning a situation where the language model behaves like a particular persona, and the user's query requires information involving three parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "NO0xoEdC2ZAy"
      },
      "outputs": [],
      "source": [
        "# YOUR CODES HERE for above practice exercise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USL19U5MKapo"
      },
      "source": [
        "#### **Example-3**\n",
        "\n",
        "The prompt to *chat models* is a list of chat messages.\n",
        "\n",
        "Each chat message is associated with content, and an additional parameter called `role`. For example, in the OpenAI Chat Completions API, a chat message can be associated with an AI assistant, a human or a system role."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "jsZWlMZEEcth"
      },
      "outputs": [],
      "source": [
        "# Import the ChatPromptTemplate class from LangChain's core prompts module to create structured chat prompts.\n",
        "from langchain_core.prompts import ChatPromptTemplate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "TBLyJ-sFk8Hf"
      },
      "outputs": [],
      "source": [
        "# Create a ChatPromptTemplate using a list of predefined message templates for system, human, and AI interactions.\n",
        "chat_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        # The system message, defining the behavior or persona of the AI.\n",
        "        (\"system\", \"You are a helpful A {persona}.\"),\n",
        "\n",
        "        # The first human message, a simple greeting.\n",
        "        (\"human\", \"Hello, how are you doing?\"),\n",
        "\n",
        "        # The AI's response to the human's greeting.\n",
        "        (\"ai\", \"I'm doing well, thanks!\"),\n",
        "\n",
        "        # The second human message, which will be dynamically filled with user input.\n",
        "        (\"human\", \"{user_input}\"),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1rr0S0bklaIr"
      },
      "outputs": [],
      "source": [
        "# Define the persona for the AI, which will be used in the system message template.\n",
        "persona = \"\"\"trustworthy friend\"\"\"\n",
        "\n",
        "# Define the user's query, asking for help with understanding a concept.\n",
        "query = \"\"\"\n",
        "I am not able to understand the concept taught in class. \\\n",
        "Could you please suggest something? \\\n",
        "I need your help. Give 5 points to work on.\n",
        "\"\"\"\n",
        "\n",
        "# Format the messages in the chat template by filling in the placeholders with the defined persona and query.\n",
        "messages = chat_template.format_messages(persona=persona, user_input=query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XoEgU1O5LlK7"
      },
      "outputs": [],
      "source": [
        "messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "aP9IQ7q8mYu8"
      },
      "outputs": [],
      "source": [
        "# Convert the formatted messages into a chat prompt that can be processed by the Hugging Face model.\n",
        "chat_model._to_chat_prompt(messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "SOm5ZjnQ24jm"
      },
      "outputs": [],
      "source": [
        "print(chat_model._to_chat_prompt(messages))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "tKH257JH_6qi"
      },
      "outputs": [],
      "source": [
        "# Invoke the chat model with the formatted messages and get the model's response.\n",
        "response = chat_model.invoke(messages)\n",
        "\n",
        "# Print the content of the response from the chat model.\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "si6NaQVggzQq"
      },
      "source": [
        "**Practice-3 :**\n",
        "Create a prompt template that takes context and a question from the user and answers the question based on the given context.\n",
        "\n",
        "Hint: Keep the context in the system message and the question in the human message.\n",
        "\n",
        "**Context:** Meet Aryan Kapoor, a rising star in the entertainment industry whose talent knows no bounds. In 2023, Aryan captivated\n",
        "audiences with his mesmerizing performance in the critically acclaimed film \"Echoes of Eternity,\" earning him the prestigious Best Actor award at the National Film Awards. His versatility shone brightly in 2024 when he showcased his vocal prowess as a playback singer in the chart-topping soundtrack of the blockbuster movie \"Infinite Horizon.\" The same year,  Aryan's captivating screen presence garnered him the coveted Filmfare Critics Award for Best Actor. As his star continued to\n",
        "ascend, Aryan was honored with the International Icon of the Year award at the Global Entertainment Awards in 2025, recognizing his global impact and widespread admiration. With each role he undertakes, Aryan Kapoor cements his status as an unrivaled  talent in the world of cinema, leaving audiences eagerly anticipating his next masterpiece.\n",
        "\n",
        "**Question:** What awards did Aryan Kapoor win for his contributions to the entertainment industry, and in which years were they received?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "I1uzhaCN1enO"
      },
      "outputs": [],
      "source": [
        "# YOUR CODES HERE for above practice exercise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "t4StVsQ7lQS7"
      },
      "outputs": [],
      "source": [
        "# Create a ChatPromptTemplate using a list of predefined message templates for system and human interactions.\n",
        "chat_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        # The system message, providing the context for the assistant and setting expectations for the response.\n",
        "        (\"system\", \"You are a helpful assistant and know this context ```{context}``\"),\n",
        "\n",
        "        # The human message, containing the query, asking for a point-based answer and to stick to the provided context.\n",
        "        (\"human\", \" pls reply ```{question}``` in points based on the context provided. Strictly don't add extra facts and information ?\"),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MN6-TQaliTMT"
      },
      "outputs": [],
      "source": [
        "# Define the user's question about Aryan Kapoor's awards and the years they were received.\n",
        "question = \"\"\"What awards did Aryan Kapoor win for his contributions to the entertainment industry,\n",
        "and in which years were they received?\"\"\"\n",
        "\n",
        "# Define the context, which provides detailed information about Aryan Kapoor's achievements.\n",
        "context = \"\"\"\n",
        "Meet Aryan Kapoor, a rising star in the entertainment industry whose talent knows no bounds. In 2023, Aryan captivated\n",
        "audiences with his mesmerizing performance in the critically acclaimed film \"Echoes of Eternity,\" earning him the\n",
        " prestigious Best Actor award at the National Film Awards. His versatility shone brightly in 2024 when he showcased his\n",
        "  vocal prowess as a playback singer in the chart-topping soundtrack of the blockbuster movie \"Infinite Horizon.\" The same year,\n",
        "  Aryan's captivating screen presence garnered him the coveted Filmfare Critics Award for Best Actor. As his star continued to\n",
        "  ascend, Aryan was honored with the International Icon of the Year award at the Global Entertainment Awards in 2025, recognizing\n",
        "   his global impact and widespread admiration. With each role he undertakes, Aryan Kapoor cements his status as an unrivaled\n",
        "   talent in the world of cinema, leaving audiences eagerly anticipating his next masterpiece.\n",
        "\"\"\"\n",
        "\n",
        "# Format the chat prompt messages by inserting the context and question into the message template.\n",
        "messages = chat_template.format_messages(context=context, question=question)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5u6PwxIqjdfn"
      },
      "outputs": [],
      "source": [
        "# Convert the formatted messages into a chat prompt that can be processed by the Hugging Face model.\n",
        "chat_model._to_chat_prompt(messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "hs-ph2ePAFb5"
      },
      "outputs": [],
      "source": [
        "# Send the formatted messages to the chat model for processing and obtain the model's response.\n",
        "response = chat_model.invoke(messages)\n",
        "\n",
        "# Print the generated response content from the chat model.\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BPY4s_hGQv-"
      },
      "source": [
        "#### **Output Parsers**\n",
        "\n",
        "Let's start with defining how we would like the LLM output to look like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "x1nyuc_1GO6u"
      },
      "outputs": [],
      "source": [
        "# An example output format\n",
        "{\n",
        "  \"gift\": False,\n",
        "  \"delivery_days\": 5,\n",
        "  \"price_value\": \"pretty affordable!\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "qW37-qdYGaOY"
      },
      "outputs": [],
      "source": [
        "customer_review = \"\"\"\\\n",
        "This leaf blower is pretty amazing.  It has four settings:\\\n",
        "candle blower, gentle breeze, windy city, and tornado. \\\n",
        "It arrived in two days, just in time for my wife's \\\n",
        "anniversary present. \\\n",
        "I think my wife liked it so much she was speechless. \\\n",
        "So far I've been the only one using it, and I've been \\\n",
        "using it every other morning to clear the leaves on our lawn. \\\n",
        "It's slightly more expensive than the other leaf blowers \\\n",
        "out there, but I think it's worth it for the extra features.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ppJOtjsEGdL_"
      },
      "outputs": [],
      "source": [
        "review_template = \"\"\"\\\n",
        "For the following text, extract the following information:\n",
        "\n",
        "gift: Was the item purchased as a gift or present for someone else? \\\n",
        "Answer True if yes, False if not or unknown.\n",
        "\n",
        "delivery_days: How many days did it take for the product \\\n",
        "to arrive? If this information is not found, output -1.\n",
        "\n",
        "price_value: Extract any sentences about the value or price,\\\n",
        "and output them as a comma separated Python list.\n",
        "\n",
        "Format the output as JSON with the following keys:\n",
        "gift\n",
        "delivery_days\n",
        "price_value\n",
        "\n",
        "text: {text}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "gQDimFP3Gp5a"
      },
      "outputs": [],
      "source": [
        "# Import ChatPromptTemplate from LangChain Core to create structured prompts for chat models.\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# Creating a chat prompt template using a predefined template string (review_template).\n",
        "prompt_template = ChatPromptTemplate.from_template(review_template)\n",
        "\n",
        "# Print the created prompt template to check its structure.\n",
        "print(prompt_template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MYur071rMz6D"
      },
      "outputs": [],
      "source": [
        "# Format the chat prompt messages by filling in the placeholder(s) in the prompt template with the actual customer review text.\n",
        "messages = prompt_template.format_messages(text=customer_review)\n",
        "\n",
        "# Send the formatted messages to the chat model for processing and obtain the model's response.\n",
        "response = chat_model.invoke(messages)\n",
        "\n",
        "# Print the generated response content from the chat model.\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cV1eVBvyHhfY"
      },
      "outputs": [],
      "source": [
        "print(type(response.content))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pf3bgzsf7pgP"
      },
      "source": [
        "#### **Parse the LLM output string into a structured data**:\n",
        "\n",
        "Language models output text. But there are times where you want to get more structured information than just text back. While some model providers support [built-in ways to return structured output](https://python.langchain.com/docs/how_to/structured_output/), not all do.\n",
        "\n",
        "Output parsers are classes that help structure language model responses.\n",
        "\n",
        "Below we go over the main type of output parser, the `PydanticOutputParser`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8N7zx3mCoIj"
      },
      "source": [
        "[Structured output parser](https://python.langchain.com/docs/how_to/output_parser_structured/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "EH0hArbY7n4t"
      },
      "outputs": [],
      "source": [
        "# Import necessary modules\n",
        "from langchain_core.output_parsers import PydanticOutputParser  # Parses model output into structured data\n",
        "from pydantic import BaseModel, Field  # Used for defining structured data models\n",
        "\n",
        "# Define the structured data model for extracting product-related information\n",
        "class Product_Info(BaseModel):\n",
        "    \"\"\"Product service info.\"\"\"\n",
        "\n",
        "    # Field to check if the product was purchased as a gift\n",
        "    gift: str = Field(description=\"Was the item purchased as a gift for someone else? \"\n",
        "                                  \"Answer 'True' if yes, 'False' if not or unknown.\")\n",
        "\n",
        "    # Field to capture the number of days taken for delivery\n",
        "    delivery_days: int = Field(description=\"How many days did it take for the product to arrive? \"\n",
        "                                           \"If this information is not found, output -1.\")\n",
        "\n",
        "    # Field to extract and store sentences related to price or value as a list\n",
        "    price_value: list = Field(description=\"Extract sentences about the value or price, \"\n",
        "                                          \"and output them as a comma-separated Python list.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "QzpGk67j24Ti"
      },
      "outputs": [],
      "source": [
        "# Set up a parser + inject instructions into the prompt template\n",
        "parser = PydanticOutputParser(pydantic_object = Product_Info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "sJp6QGTF3Ma7"
      },
      "outputs": [],
      "source": [
        "# Import PromptTemplate from LangChain to create structured prompts for the model\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Create a prompt template to guide the model's response generation\n",
        "prompt = PromptTemplate(\n",
        "    template=\"Answer the user query.\\n{format_instructions}\\n{text}\\n\",  # Template structure\n",
        "    input_variables=[\"text\"],  # Defines 'text' as a required input variable\n",
        "    partial_variables={\"format_instructions\": parser.get_format_instructions()},  # Inserts format instructions dynamically\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "kLPLDAGm9c0F"
      },
      "outputs": [],
      "source": [
        "# Create a pipeline where the formatted prompt is passed to the language model (llm)\n",
        "prompt_and_model = prompt | llm  # The \"|\" operator chains the prompt and the LLM for execution\n",
        "\n",
        "# Invoke the pipeline with a user query (customer review) to generate a structured response\n",
        "output = prompt_and_model.invoke({\"text\": customer_review})\n",
        "\n",
        "# Parse the model's output into the predefined structured format using the parser\n",
        "result = parser.invoke(output)\n",
        "\n",
        "# Display the final structured result\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "u3BgDcCa7tcI"
      },
      "outputs": [],
      "source": [
        "print(result.gift)\n",
        "print(result.delivery_days)\n",
        "print(result.price_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uG22QIv3s_p9"
      },
      "source": [
        "**Practice-4 :** Continuing the practice-3, can you get the ouput in the below format :\n",
        "\n",
        "{'Year': 'Award}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "fNQ4yrNH4x_U"
      },
      "outputs": [],
      "source": [
        "# YOUR CODES HERE for above practice exercise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Br30bwaPKs0N"
      },
      "source": [
        "#### [**Customizing Conversational Memory**](https://python.langchain.com/docs/how_to/chatbots_memory/)\n",
        "\n",
        "LangChain can helps in building better chatbots, or have\n",
        "an LLM with more effective chats by better managing\n",
        "what it remembers from the conversation you've had so far."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "TBDcA56ovm_w"
      },
      "outputs": [],
      "source": [
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are a helpful assistant. Answer all questions to the best of your ability.\",\n",
        "        ),\n",
        "        (\"placeholder\", \"{chat_history}\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain = prompt | chat_model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "C1_WmMeZv1HV"
      },
      "outputs": [],
      "source": [
        "# Import ChatMessageHistory to manage and store conversation history\n",
        "from langchain_community.chat_message_histories import ChatMessageHistory\n",
        "\n",
        "# Import RunnableWithMessageHistory to enable conversation-aware execution of chat models\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-zGGnVncvtWt"
      },
      "outputs": [],
      "source": [
        "# Dictionary to store chat message histories for different sessions\n",
        "store = {}\n",
        "\n",
        "# Function to retrieve or create a new chat history for a given session ID\n",
        "def get_session_history(session_id: str) -> ChatMessageHistory:\n",
        "    if session_id not in store:\n",
        "        store[session_id] = ChatMessageHistory()  # Initialize new history if session does not exist\n",
        "    return store[session_id]  # Return the existing or new session history\n",
        "\n",
        "# Create a chat pipeline with message history tracking\n",
        "chain_with_message_history = RunnableWithMessageHistory(\n",
        "    runnable=chain,  # The main conversation chain (LLM model + prompt)\n",
        "    get_session_history=get_session_history,  # Function to retrieve chat history per session\n",
        "    input_messages_key=\"input\",  # Key in the input where the new user message is stored\n",
        "    history_messages_key=\"chat_history\",  # Key in the input where the previous chat history is stored\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTe4OV8Kwoat"
      },
      "outputs": [],
      "source": [
        "# Invoke the chat model with message history tracking\n",
        "chain_with_message_history.invoke(\n",
        "    {\"input\": \"Hi, my name is James\"},  # User's input message\n",
        "    {\"configurable\": {\"session_id\": \"user1\"}},  # Configuration containing session ID\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ALK5ZboQw53S"
      },
      "outputs": [],
      "source": [
        "store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HQPxFgMEwz0u"
      },
      "outputs": [],
      "source": [
        "# Invoke the chat model with message history tracking for a specific user session\n",
        "chain_with_message_history.invoke(\n",
        "    input={\"input\": \"Do you remember my name?\"},  # User's current input message\n",
        "    config={\"configurable\": {\"session_id\": \"user1\"}}  # Session configuration to maintain chat history\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vsmS0nLgxdxe"
      },
      "outputs": [],
      "source": [
        "store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gbW0TdOVxef3"
      },
      "outputs": [],
      "source": [
        "# Invoke the chat model with message history tracking to retrieve the user's name\n",
        "chain_with_message_history.invoke(\n",
        "    input={\"input\": \"Can you tell me what is my name?\"},  # User's current query asking for their name\n",
        "    config={\"configurable\": {\"session_id\": \"user1\"}}  # Session configuration to ensure chat history for \"user1\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wb2HR0MAvm3j"
      },
      "outputs": [],
      "source": [
        "store"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSbCSUOfxXa-"
      },
      "source": [
        "### **II. mistralai/Mistral-7B-Instruct-v0.2**\n",
        "\n",
        "Note that you need to ask for access before using this model. Go to https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2 and click on `Agree and access repository`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_733Aai1RG3"
      },
      "outputs": [],
      "source": [
        "# Importing HuggingFaceEndpoint from langchain_huggingface to interact with Hugging Face models via an API endpoint\n",
        "from langchain_huggingface import HuggingFaceEndpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_XZllnjBzqw0"
      },
      "outputs": [],
      "source": [
        "question = \"How to learn programing? Give 5 examples. \""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bNrWpxyu5yxP"
      },
      "outputs": [],
      "source": [
        "# Set the Hugging Face model repository ID\n",
        "repo_id = \"mistralai/Mistral-7B-Instruct-v0.2\"  # Model identifier for Mistral-7B\n",
        "\n",
        "# Define additional model parameters, including max token length and authentication token\n",
        "model_kwargs = {\n",
        "    \"max_length\": 128,  # Maximum token length for generated output\n",
        "    \"token\": os.environ[\"HF_TOKEN\"]  # Hugging Face API token for authentication, stored in environment variables\n",
        "}\n",
        "\n",
        "# Initialize the HuggingFaceEndpoint to interact with the model hosted on Hugging Face\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=repo_id,  # Model repo ID to specify which model to use\n",
        "    task=\"text-generation\",  # Task to perform, here it's text generation (common for LLMs)\n",
        "    temperature=0.5,  # Temperature parameter controls the randomness of the output (higher value = more random)\n",
        "    model_kwargs=model_kwargs  # Pass the additional model parameters (max length, token)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bXJMBgfy0A5Q"
      },
      "outputs": [],
      "source": [
        "# Use the Hugging Face model endpoint to generate a response based on the input question\n",
        "response = llm.invoke(question)  # Pass the user's question to the model for text generation\n",
        "print(response)  # Output the generated response from the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmRI-p9fp0SR"
      },
      "source": [
        "#### **Prompt Template**\n",
        "\n",
        "**Example-1**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xiZNTCsGNpPC"
      },
      "outputs": [],
      "source": [
        "# Importing message types from langchain.schema to represent different types of messages in a conversation\n",
        "from langchain.schema import (\n",
        "    HumanMessage,  # Represents a message from the user (human)\n",
        "    SystemMessage,  # Represents a message from the system, providing context or instructions\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WnF-Qc_FrTZV"
      },
      "outputs": [],
      "source": [
        "# Importing the ChatPromptTemplate class from langchain_core.prompts to create and format chat-based prompts\n",
        "from langchain_core.prompts import ChatPromptTemplate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHJWqMg3yxa_"
      },
      "outputs": [],
      "source": [
        "template_s = \"\"\"You are a {style1}.\\\n",
        "Tell me  {count} facts about {event_or_place}.```\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wHucwPZ7yVPF"
      },
      "outputs": [],
      "source": [
        "# Creating a ChatPromptTemplate using a template string, where 'template_s' is a predefined prompt template\n",
        "prompt_template = ChatPromptTemplate.from_template(template_s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jH_lqJRbzaHt"
      },
      "outputs": [],
      "source": [
        "# Accessing the prompt of the first message in the ChatPromptTemplate\n",
        "prompt_template.messages[0].prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fDyBeGD8zej9"
      },
      "outputs": [],
      "source": [
        "# Accessing the input variables of the first message's prompt in the ChatPromptTemplate\n",
        "prompt_template.messages[0].prompt.input_variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QXUiH0sszkUe"
      },
      "outputs": [],
      "source": [
        "# Formatting the messages using the prompt_template with specific values for placeholders\n",
        "user_messages = prompt_template.format_messages(\n",
        "    style1=\"knowledgeable historian\",  # Replacing the 'style1' placeholder with the value \"knowledgeable historian\"\n",
        "    count=5,                           # Replacing the 'count' placeholder with the value 5\n",
        "    event_or_place=\"Tajmahal\"          # Replacing the 'event_or_place' placeholder with the value \"Tajmahal\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qP62G5bz0KBJ"
      },
      "outputs": [],
      "source": [
        "user_messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5IfurZcyYt8"
      },
      "outputs": [],
      "source": [
        "# Importing ChatHuggingFace from langchain_huggingface to interface with Hugging Face models\n",
        "from langchain_huggingface import ChatHuggingFace\n",
        "\n",
        "# Creating an instance of ChatHuggingFace using a previously defined language model (llm)\n",
        "chat_model = ChatHuggingFace(llm=llm)\n",
        "\n",
        "# Accessing the model ID associated with the chat_model instance\n",
        "chat_model.model_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ybxEq_9JqywX"
      },
      "outputs": [],
      "source": [
        "# Converting the user messages to a chat-friendly format for the chat_model\n",
        "chat_model._to_chat_prompt(user_messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SYI0aD8Hsty_"
      },
      "outputs": [],
      "source": [
        "# Invoking the chat model with user messages to generate a response\n",
        "response = chat_model.invoke(user_messages)\n",
        "\n",
        "# Printing the content of the response generated by the chat model\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wakjVUVZp2YH"
      },
      "source": [
        "**Example-2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jiFDpiyENyOS"
      },
      "outputs": [],
      "source": [
        "# Creating a list of messages with a single human message as input\n",
        "messages = [HumanMessage(content=\"How to learn programming? give 5 points\")]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6SO2qIrju7rc"
      },
      "outputs": [],
      "source": [
        "# Converting the list of messages to the appropriate chat format for the chat model\n",
        "chat_model._to_chat_prompt(messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jEhKW2msODT2"
      },
      "outputs": [],
      "source": [
        "# Sending the formatted messages to the chat model and generating a response\n",
        "response = chat_model.invoke(messages)\n",
        "\n",
        "# Printing the content of the response generated by the chat model\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaaC7-gu_KOK"
      },
      "source": [
        "### **III.** **[Llama2](https://ai.meta.com/llama/)** ***(Optional)***\n",
        "\n",
        "**NOTE:**\n",
        "\n",
        ">For using this model you have to click `Download models` link available in [this](https://ai.meta.com/llama/) reference which re-direct to a **form for request**. It may take 1 hour to 2 days to get the **approval** for usage of this model through HuggingFace. You will get an email for the same.\n",
        "\n",
        ">Once the request is approved, connect to **GPU runtime** for below steps. Also, you need to provide your HF api key/access token.\n",
        "\n",
        "Trying Llama2-2-7b model:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HbzbpWIK_J4T"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# This is a Jupyter notebook magic command that captures the output of the cell,\n",
        "# preventing it from being printed in the notebook.\n",
        "\n",
        "!pip install -q transformers accelerate langchain xformers bitsandbytes\n",
        "# Using pip to install the following Python packages:\n",
        "# - transformers: A library for working with transformer models like GPT, BERT, etc.\n",
        "# - accelerate: A library to speed up the training and inference of large models efficiently.\n",
        "# - langchain: A framework for building applications using large language models (LLMs).\n",
        "# - xformers: Tools and optimizations for efficient memory usage and performance in transformers.\n",
        "# - bitsandbytes: A library for running models with low-bit precision to improve memory usage and speed.\n",
        "# The '-q' flag ensures the installation process runs quietly without verbose output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zW69WOWSzwkG"
      },
      "outputs": [],
      "source": [
        "# Enter your HuggingFace access token when prompted\n",
        "\n",
        "import os  # Importing the 'os' module for interacting with the operating system\n",
        "from getpass import getpass  # Importing 'getpass' to securely input the HuggingFace token without displaying it\n",
        "\n",
        "# Prompt the user to input their HuggingFace access token without showing the input\n",
        "pass_token = getpass(\"Enter your HuggingFace access token: \")\n",
        "\n",
        "# Setting the HuggingFace token as environment variables to be used in API requests\n",
        "os.environ[\"HF_TOKEN\"] = pass_token  # Storing the token as 'HF_TOKEN' in the environment\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = pass_token  # Storing the token as 'HUGGINGFACEHUB_API_TOKEN'\n",
        "\n",
        "# Delete the pass_token variable for security reasons to ensure it doesn't remain in memory\n",
        "del pass_token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rw7oXd2pAE1f"
      },
      "source": [
        "## Initializing the Hugging Face Pipeline\n",
        "\n",
        "The first thing we need to do is initialize a `text-generation` pipeline with Hugging Face transformers. The Pipeline requires three things that we must initialize first, those are:\n",
        "\n",
        "* A LLM, in this case it will be `meta-llama/Llama-2-7b-chat-hf`.\n",
        "\n",
        "* The respective tokenizer for the model.\n",
        "\n",
        "We'll explain these as we get to them, let's begin with our model.\n",
        "\n",
        "We initialize the model and move it to our CUDA-enabled GPU. Using Colab this can take 5-10 minutes to download and initialize the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--40_VMGJBEz"
      },
      "outputs": [],
      "source": [
        "from torch import cuda, bfloat16\n",
        "# Importing 'cuda' from PyTorch to interact with GPU for acceleration (e.g., checking for available CUDA devices, using the GPU for tensor operations).\n",
        "# Importing 'bfloat16' from PyTorch, a 16-bit floating point format often used to save memory and speed up computation, especially in deep learning.\n",
        "\n",
        "import transformers\n",
        "# Importing the 'transformers' library from Hugging Face, which provides tools for working with transformer models like GPT, BERT, and others."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h49z-C1RJC9D"
      },
      "outputs": [],
      "source": [
        "# Define the model ID for the pre-trained model you want to use from Hugging Face\n",
        "model_id = 'meta-llama/Llama-2-7b-chat-hf'\n",
        "# 'meta-llama/Llama-2-7b-chat-hf' is the ID of a pre-trained Llama-2 model hosted on Hugging Face.\n",
        "# This model is fine-tuned for chat applications.\n",
        "\n",
        "# Determine which device (GPU or CPU) to use for model inference\n",
        "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
        "# If a CUDA-enabled GPU is available, use the first available GPU (cuda:0).\n",
        "# If CUDA is not available, use the CPU ('cpu').\n",
        "device  # This stores the device configuration for later use when loading the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2Cp6jnomS0S"
      },
      "outputs": [],
      "source": [
        "# Load the tokenizer for the model specified by 'model_id' from Hugging Face\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(model_id)\n",
        "# 'AutoTokenizer' automatically selects the appropriate tokenizer based on the model provided.\n",
        "# The tokenizer is responsible for converting text input into token IDs that the model can understand.\n",
        "\n",
        "# Set up a text-generation pipeline using the specified model and tokenizer\n",
        "pipeline = transformers.pipeline(\n",
        "    \"text-generation\",  # Specifies the type of task (text generation in this case)\n",
        "    model=model_id,  # The model to use for text generation (in this case, Llama-2 model)\n",
        "    tokenizer=tokenizer,  # Tokenizer to convert input text into tokens\n",
        "    torch_dtype=bfloat16,  # Use 'bfloat16' precision for tensor computations to reduce memory usage and speed up computations\n",
        "    trust_remote_code=True,  # Trust remote code when loading model weights and configurations\n",
        "    device_map=\"auto\",  # Automatically map the model layers to available devices (CPU/GPU)\n",
        "    max_length=1000,  # Maximum length of the generated text (number of tokens)\n",
        "    do_sample=True,  # Enable sampling for text generation (instead of deterministic outputs)\n",
        "    top_k=10,  # Limits the sampling to the top 10 most probable next tokens to ensure diversity\n",
        "    num_return_sequences=1,  # Number of generated sequences to return\n",
        "    eos_token_id=tokenizer.eos_token_id  # End-of-sequence token ID, so generation stops when this token is produced\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8n7MjpX4nM-r"
      },
      "outputs": [],
      "source": [
        "# Pass the input prompt to the pipeline and generate text based on the given prompt\n",
        "res = pipeline(\"How to learn programming?\")\n",
        "# The pipeline processes the input prompt (\"How to learn programming?\") and generates text.\n",
        "# The result is stored in the variable 'res'. The pipeline returns a list of generated sequences.\n",
        "\n",
        "# Print the generated text from the first sequence\n",
        "print(res[0][\"generated_text\"])\n",
        "# The generated text is accessed from the first result in the list (res[0]).\n",
        "# 'generated_text' is the key that contains the model's output for the generated text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzqImT9RAdAg"
      },
      "source": [
        "#### **Now implementing with LangChain**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLkLetCxWF-t"
      },
      "outputs": [],
      "source": [
        "# Install the LangChain library, a framework for building language model-based applications\n",
        "!pip -q install langchain\n",
        "# The '-q' flag suppresses unnecessary output, making the installation process less verbose.\n",
        "# LangChain is useful for building chains of various language model components like prompts, tools, and memory.\n",
        "\n",
        "# Install the langchain_community library, which contains community-supported modules for LangChain\n",
        "!pip -q install langchain_community\n",
        "# This library includes additional modules contributed by the LangChain community to extend its functionality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h71jByoBAffB"
      },
      "outputs": [],
      "source": [
        "# Import HuggingFacePipeline from langchain.llms\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "\n",
        "# Create an LLM instance by wrapping the HuggingFace pipeline inside LangChain\n",
        "# The pipeline processes the input data and generates responses.\n",
        "llm = HuggingFacePipeline(pipeline=pipeline, model_kwargs={'temperature': 0.7})\n",
        "# 'pipeline': The pre-configured HuggingFace pipeline used to generate responses based on input prompts.\n",
        "# 'model_kwargs': Additional keyword arguments passed to the model, in this case, setting the 'temperature' to 0.7.\n",
        "# 'temperature': A parameter that controls the randomness of the output (higher values lead to more randomness)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2tr-YGpvnr0X"
      },
      "outputs": [],
      "source": [
        "# Invoke the HuggingFace model wrapped inside the LangChain LLM with the query \"How to learn programming?\"\n",
        "# The 'invoke' method runs the input query through the model pipeline and generates a response.\n",
        "print(llm.invoke(\"How to learn programming?\"))\n",
        "# This will output the response generated by the model to the prompt \"How to learn programming?\".\n",
        "# The model will process the query and provide an answer based on its training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prWXot8wcKyS"
      },
      "source": [
        "#### **Prompt Template**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7DTl7I1HaOJK"
      },
      "outputs": [],
      "source": [
        "# Importing ChatPromptTemplate from langchain.prompts\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "# This class allows for creating templates for chat-based prompts. You can define the structure of messages in the chat,\n",
        "# and then dynamically replace parts of it based on inputs or parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mqU1EAehaOJP"
      },
      "outputs": [],
      "source": [
        "# Define a multi-line string template for a chat prompt using placeholders for style and text\n",
        "template_s = \"\"\"Reply the answer\n",
        "like {style1}.\n",
        "text: ```{text1}```\n",
        "\"\"\"\n",
        "# This string will be used as a template, where:\n",
        "# {style1}: Placeholder for the style in which the answer should be given (e.g., \"knowledgeable historian\").\n",
        "# {text1}: Placeholder for the text or query to which the model will respond."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YK6fMe1paOJP"
      },
      "outputs": [],
      "source": [
        "# Create a ChatPromptTemplate instance using the previously defined template\n",
        "prompt_template = ChatPromptTemplate.from_template(template_s)\n",
        "# This initializes the ChatPromptTemplate with the string 'template_s' as its format.\n",
        "# The template will later allow dynamic substitution of the placeholders {style1} and {text1}."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R5AlexzxaOJP"
      },
      "outputs": [],
      "source": [
        "# Access the first message in the prompt template and retrieve its prompt string\n",
        "prompt_template.messages[0].prompt\n",
        "# This accesses the first message in the 'messages' list of the ChatPromptTemplate,\n",
        "# and retrieves the actual prompt string (i.e., the template structure defined earlier)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJa3nBfbaOJQ"
      },
      "outputs": [],
      "source": [
        "# Access the input variables that are used in the prompt template\n",
        "prompt_template.messages[0].prompt.input_variables\n",
        "# This retrieves the list of input variables (placeholders) used in the first message's prompt template.\n",
        "# In this case, the input variables are {style1} and {text1}, which are placeholders for dynamic content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYlQrFsnaOJQ"
      },
      "outputs": [],
      "source": [
        "# Define the style to be used in the prompt template\n",
        "style = \"\"\"trustworthy friend\"\"\"\n",
        "# This assigns the string \"trustworthy friend\" to the variable 'style'.\n",
        "# This value will be used as the persona or tone in which the response should be framed in the prompt template."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D9UtT3reaOJQ"
      },
      "outputs": [],
      "source": [
        "# Define the query to be used in the prompt template\n",
        "query = \"\"\"\n",
        "I am not able to understand the concept taught in class.\n",
        "Could you please suggest something?\n",
        "I need your help. Give 5 points to work on.\n",
        "\"\"\"\n",
        "# This variable 'query' holds the question or request that will be input to the language model.\n",
        "# It's a message asking for help with understanding a concept and requesting 5 points to work on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AUB4HHQTaOJQ"
      },
      "outputs": [],
      "source": [
        "# Format the messages using the prompt template, filling in the placeholders\n",
        "user_messages = prompt_template.format_messages(\n",
        "    style1=style,   # Provide the style (e.g., \"trustworthy friend\")\n",
        "    text1=query     # Provide the user query (e.g., the request for help with 5 points)\n",
        ")\n",
        "# This line fills the placeholders {style1} and {text1} in the prompt template with the defined values\n",
        "# of 'style' (persona) and 'query' (the actual question). The resulting 'user_messages' will be used\n",
        "# to generate the final input for the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6r6WC6OZaOJQ"
      },
      "outputs": [],
      "source": [
        "# Print the first formatted message from the user_messages list\n",
        "print(user_messages[0])\n",
        "# This will output the first message in the 'user_messages' list after formatting.\n",
        "# The output will be a string where the placeholders {style1} and {text1} have been replaced by the values of 'style' and 'query'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-zPBqhKaOJQ"
      },
      "outputs": [],
      "source": [
        "# Call the LLM to translate to the style of the customer message\n",
        "llm_response = llm.invoke(user_messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2sMDbkrmoIs_"
      },
      "outputs": [],
      "source": [
        "print(llm_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErjQyyi4nR2n"
      },
      "source": [
        "### Please answer the questions below to complete the experiment:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VgSwVENIPcM6"
      },
      "outputs": [],
      "source": [
        "#@title Which of the following prompt techniques in LangChain allows flexible templated prompts that are suitable for better describing the role and content? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Answer = \"\" #@param [\"\", \"PromptTemplate\", \"ChatPromptTemplate\", \"Both\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMzKSbLIgFzQ"
      },
      "outputs": [],
      "source": [
        "#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Complexity = \"\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DjcH1VWSFI2l"
      },
      "outputs": [],
      "source": [
        "#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n",
        "Additional = \"\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4VBk_4VTAxCM"
      },
      "outputs": [],
      "source": [
        "#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Concepts = \"\" #@param [\"\",\"Yes\", \"No\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XH91cL1JWH7m"
      },
      "outputs": [],
      "source": [
        "#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Comments = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8xLqj7VWIKW"
      },
      "outputs": [],
      "source": [
        "#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Mentor_support = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "FzAZHt1zw-Y-"
      },
      "outputs": [],
      "source": [
        "#@title Run this cell to submit your notebook for grading { vertical-output: true }\n",
        "try:\n",
        "  if submission_id:\n",
        "      return_id = submit_notebook()\n",
        "      if return_id : submission_id = return_id\n",
        "  else:\n",
        "      print(\"Please complete the setup first.\")\n",
        "except NameError:\n",
        "  print (\"Please complete the setup first.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}