{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amsac/ML_Notebooks/blob/main/NLP/M3_AST_01_TextPreprocessing_using_spaCy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced Certification Programme in AI and MLOps\n",
        "## A programme by IISc and TalentSprint\n",
        "### Assignment 1: Text Preprocessing using spaCy"
      ],
      "metadata": {
        "id": "zSstSEIv155v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Learning Objectives:\n",
        "\n",
        "At the end of the experiment, you will be able to:\n",
        "\n",
        "* understand the spaCy library\n",
        "* perform simple natural language processing tasks using the spaCy library"
      ],
      "metadata": {
        "id": "akVZjBjD2PtD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction"
      ],
      "metadata": {
        "id": "VGel018GxQGn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**spaCy** is a free, open-source library for advanced Natural Language Processing (NLP) in Python.\n",
        "\n",
        "It is designed specifically for production use and helps you build applications that process and “understand” large volumes of text. It can be used to build information extraction or natural language understanding systems, or to pre-process text for deep learning.\n",
        "\n",
        "spaCy's features and capabilities include:\n",
        "\n",
        "- ***Tokenization***:\tSegmenting text into words, punctuations marks etc.\n",
        "- ***Part-of-speech (POS) Tagging***: Assigning word types to tokens, like verb or noun.\n",
        "- ***Dependency Parsing***: Assigning syntactic dependency labels, describing the relations between individual tokens, like subject or object.\n",
        "- ***Lemmatization***: Assigning the base forms of words. For example, the lemma of “was” is “be”, and the lemma of “rats” is “rat”.\n",
        "- ***Sentence Boundary Detection (SBD)***: Finding and segmenting individual sentences.\n",
        "- ***Named Entity Recognition (NER)***: Labelling named “real-world” objects, like persons, companies or locations.\n",
        "- ***Entity Linking (EL)***: Disambiguating textual entities to unique identifiers in a knowledge base.\n",
        "- ***Similarity***: Comparing words, text spans and documents and how similar they are to each other.\n",
        "- ***Text Classification***: Assigning categories or labels to a whole document, or parts of a document.\n",
        "- ***Rule-based Matching***: Finding sequences of tokens based on their texts and linguistic annotations, similar to regular expressions.\n",
        "- ***Training***: Updating and improving a statistical model's predictions.\n",
        "- ***Serialization***: Saving objects to files or byte strings.\n"
      ],
      "metadata": {
        "id": "TI_iPJtzxSEB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Statistical models\n",
        "\n",
        "While some of spaCy's features work independently, others require ***trained pipelines*** to be loaded, which enable spaCy to predict linguistic annotations - for example, whether a word is a verb or a noun.\n",
        "\n",
        "A trained pipeline can consist of multiple components that use a statistical model trained on labeled data.\n",
        "\n",
        "spaCy currently offers trained pipelines for a variety of languages, which can be installed as individual Python modules. Pipeline packages can differ in size, speed, memory usage, accuracy and the data they include.\n",
        "\n",
        "For English language, available trained pipelines include:\n",
        "- `en_core_web_sm`\n",
        "- `en_core_web_md`\n",
        "- `en_core_web_lg`\n",
        "- `en_core_web_trf` - English transformer pipeline\n",
        "\n",
        "To know more about trained pipelines for English, refer [here](https://spacy.io/models/en).\n",
        "\n",
        "Let's perform basic NLP tasks with spaCy using an English trained pipeline.\n"
      ],
      "metadata": {
        "id": "k9SWBYOydg-e"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vYQQZ4j8duG"
      },
      "source": [
        "### Setup Steps:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2YzfoPvJDiTX"
      },
      "outputs": [],
      "source": [
        "#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n",
        "Id = \"2400572\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "AjoZJWGErxGf"
      },
      "outputs": [],
      "source": [
        "#@title Please enter your password (your registered phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n",
        "password = \"9605144576\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "WBPPuGmBlDIN",
        "collapsed": true,
        "outputId": "54ede14e-0f41-4f7b-91d3-17d2a1237ce1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId=2400572&recordId=3430\"></script>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup completed successfully\n"
          ]
        }
      ],
      "source": [
        "#@title Run this cell to complete the setup for this Notebook\n",
        "from IPython import get_ipython\n",
        "\n",
        "ipython = get_ipython()\n",
        "\n",
        "notebook= \"M3_AST_01_TextPreprocessing_using_spaCy\" #name of the notebook\n",
        "\n",
        "def setup():\n",
        "#  ipython.magic(\"sx pip3 install torch\")\n",
        "\n",
        "    # ipython.magic(\"wget https://cdn.iisc.talentsprint.com/AIandMLOps/Datasets/Acoustic_Extinguisher_Fire_Dataset.xlsx\")\n",
        "    from IPython.display import HTML, display\n",
        "    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n",
        "    print(\"Setup completed successfully\")\n",
        "    return\n",
        "\n",
        "def submit_notebook():\n",
        "    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n",
        "\n",
        "    import requests, json, base64, datetime\n",
        "\n",
        "    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n",
        "    if not submission_id:\n",
        "      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "\n",
        "      if r[\"status\"] == \"Success\":\n",
        "          return r[\"record_id\"]\n",
        "      elif \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print (\"Something is wrong, the notebook will not be submitted for grading\")\n",
        "        return None\n",
        "\n",
        "    elif getAnswer() and getComplexity() and getAdditional() and getConcepts() and getComments() and getMentorSupport():\n",
        "      f = open(notebook + \".ipynb\", \"rb\")\n",
        "      file_hash = base64.b64encode(f.read())\n",
        "\n",
        "      data = {\"complexity\" : Complexity, \"additional\" :Additional,\n",
        "              \"concepts\" : Concepts, \"record_id\" : submission_id,\n",
        "              \"answer\" : Answer, \"id\" : Id, \"file_hash\" : file_hash,\n",
        "              \"notebook\" : notebook,\n",
        "              \"feedback_experiments_input\" : Comments,\n",
        "              \"feedback_mentor_support\": Mentor_support}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "      if \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print(\"Your submission is successful.\")\n",
        "        print(\"Ref Id:\", submission_id)\n",
        "        print(\"Date of submission: \", r[\"date\"])\n",
        "        print(\"Time of submission: \", r[\"time\"])\n",
        "        print(\"View your submissions: https://aimlops-iisc.talentsprint.com/notebook_submissions\")\n",
        "        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n",
        "        return submission_id\n",
        "    else: submission_id\n",
        "\n",
        "\n",
        "def getAdditional():\n",
        "  try:\n",
        "    if not Additional:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Additional\n",
        "  except NameError:\n",
        "    print (\"Please answer Additional Question\")\n",
        "    return None\n",
        "\n",
        "def getComplexity():\n",
        "  try:\n",
        "    if not Complexity:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Complexity\n",
        "  except NameError:\n",
        "    print (\"Please answer Complexity Question\")\n",
        "    return None\n",
        "\n",
        "def getConcepts():\n",
        "  try:\n",
        "    if not Concepts:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Concepts\n",
        "  except NameError:\n",
        "    print (\"Please answer Concepts Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "# def getWalkthrough():\n",
        "#   try:\n",
        "#     if not Walkthrough:\n",
        "#       raise NameError\n",
        "#     else:\n",
        "#       return Walkthrough\n",
        "#   except NameError:\n",
        "#     print (\"Please answer Walkthrough Question\")\n",
        "#     return None\n",
        "\n",
        "def getComments():\n",
        "  try:\n",
        "    if not Comments:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Comments\n",
        "  except NameError:\n",
        "    print (\"Please answer Comments Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getMentorSupport():\n",
        "  try:\n",
        "    if not Mentor_support:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Mentor_support\n",
        "  except NameError:\n",
        "    print (\"Please answer Mentor support Question\")\n",
        "    return None\n",
        "\n",
        "def getAnswer():\n",
        "  try:\n",
        "    if not Answer:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Answer\n",
        "  except NameError:\n",
        "    print (\"Please answer Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getId():\n",
        "  try:\n",
        "    return Id if Id else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "def getPassword():\n",
        "  try:\n",
        "    return password if password else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "submission_id = None\n",
        "### Setup\n",
        "if getPassword() and getId():\n",
        "  submission_id = submit_notebook()\n",
        "  if submission_id:\n",
        "    setup()\n",
        "else:\n",
        "  print (\"Please complete Id and Password cells before running setup\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install packages"
      ],
      "metadata": {
        "id": "ysivBdlXMxoe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the specific version 3.7.4 of the SpaCy library quietly (without showing unnecessary logs).\n",
        "!pip -q install spacy==3.7.4"
      ],
      "metadata": {
        "id": "RnmSjKExJ-AK",
        "outputId": "1e1d316f-8a80-4e5c-86d6-85e02a41a355",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/6.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/6.6 MB\u001b[0m \u001b[31m65.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m105.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m68.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/57.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/46.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.0/45.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the installation details of SpaCy, such as version, available models, and configuration, by running the SpaCy CLI command.\n",
        "!python -m spacy info"
      ],
      "metadata": {
        "id": "NlHYHxmhJWqC",
        "outputId": "ba3af622-8edf-4e31-bc49-7e5a55c7f26e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\n",
            "============================== Info about spaCy ==============================\u001b[0m\n",
            "\n",
            "spaCy version    3.7.4                         \n",
            "Location         /usr/local/lib/python3.10/dist-packages/spacy\n",
            "Platform         Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "Python version   3.10.12                       \n",
            "Pipelines        en_core_web_sm (3.7.1)        \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above info, we can see that by default spaCy contains the small trained pipeline for English `en_core_web_sm`.\n",
        "\n",
        "To use medium, large, and transformer trained pipelines, they need to be installed first using the `!python -m spacy download` command.\n",
        "\n",
        "For example: `!python -m spacy download en_core_web_trf`"
      ],
      "metadata": {
        "id": "ioo8J8kk270t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install English transformer pipeline\n",
        "# Note that Runtime needs to restart after this step\n",
        "\n",
        "!python -m spacy download en_core_web_trf"
      ],
      "metadata": {
        "id": "enIOTr8y6hf9",
        "outputId": "0970f036-cc44-452b-b68b-48be4f9d424d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_trf-3.7.3/en_core_web_trf-3.7.3-py3-none-any.whl (457.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m457.4/457.4 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-trf==3.7.3) (3.7.4)\n",
            "Collecting spacy-curated-transformers<0.3.0,>=0.2.0 (from en-core-web-trf==3.7.3)\n",
            "  Downloading spacy_curated_transformers-0.2.2-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (1.0.11)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (2.0.10)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (2.5.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (2.10.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (1.26.4)\n",
            "Collecting curated-transformers<0.2.0,>=0.1.0 (from spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3)\n",
            "  Downloading curated_transformers-0.1.1-py2.py3-none-any.whl.metadata (965 bytes)\n",
            "Collecting curated-tokenizers<0.1.0,>=0.0.9 (from spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3)\n",
            "  Downloading curated_tokenizers-0.0.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: torch>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3) (2.5.1+cu121)\n",
            "Requirement already satisfied: regex>=2022 in /usr/local/lib/python3.10/dist-packages (from curated-tokenizers<0.1.0,>=0.0.9->spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3) (2024.11.6)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (2024.12.14)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (0.1.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3) (3.4.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3) (1.3.0)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (1.2.1)\n",
            "Downloading spacy_curated_transformers-0.2.2-py2.py3-none-any.whl (236 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.3/236.3 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading curated_tokenizers-0.0.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (731 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.6/731.6 kB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading curated_transformers-0.1.1-py2.py3-none-any.whl (25 kB)\n",
            "Installing collected packages: curated-tokenizers, curated-transformers, spacy-curated-transformers, en-core-web-trf\n",
            "Successfully installed curated-tokenizers-0.0.9 curated-transformers-0.1.1 en-core-web-trf-3.7.3 spacy-curated-transformers-0.2.2\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_trf')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Restart the Runtime/Session**"
      ],
      "metadata": {
        "id": "HpB5KKWSKkby"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy info"
      ],
      "metadata": {
        "id": "x83z3TOfNGS-",
        "outputId": "027b679b-ae70-4a76-cce3-a6bd8ccded16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\n",
            "============================== Info about spaCy ==============================\u001b[0m\n",
            "\n",
            "spaCy version    3.7.4                         \n",
            "Location         /usr/local/lib/python3.10/dist-packages/spacy\n",
            "Platform         Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "Python version   3.10.12                       \n",
            "Pipelines        en_core_web_trf (3.7.3), en_core_web_sm (3.7.1)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import required packages"
      ],
      "metadata": {
        "id": "d6wPl_vwqL6Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "FFOiNNeP0giN"
      },
      "outputs": [],
      "source": [
        "# Import the SpaCy library to perform NLP tasks and its visualization tool, displaCy, for rendering annotations and dependency parses.\n",
        "import spacy  # The main SpaCy library for Natural Language Processing (NLP).\n",
        "from spacy import displacy  # displacy is a SpaCy module used for visualizing text annotations like entity recognition and syntactic dependencies."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the trained pipeline\n",
        "\n",
        "Once you've downloaded and installed a trained pipeline, you can load it via `spacy.load()`. This will return a *Language object* containing all components and data needed to process text. We usually call it `nlp`.\n"
      ],
      "metadata": {
        "id": "OoRmCXB55X3l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load transformer pipeline for English\n",
        "nlp = spacy.load(\"en_core_web_trf\")\n",
        "\n",
        "# This gives us a Language object\n",
        "nlp"
      ],
      "metadata": {
        "id": "GGlQY8Ro2hw6",
        "outputId": "c385e825-57c0-4993-a9bb-109f2c2a8384",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/thinc/shims/pytorch.py:253: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(filelike, map_location=device))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<spacy.lang.en.English at 0x7bfdca4626e0>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esentially, spaCy's *Language* object is a pipeline that uses the language model to perform a number of natural language processing tasks such as *tokenization*, *part-of-speech tagging*, *syntactic parsing*, *named entity recognition*, etc.\n",
        "\n",
        "<br>\n",
        "<img src='https://cdn.iisc.talentsprint.com/AIandMLOps/Images/spacy_pipeline.png' width=800px>\n",
        "\n",
        "<br>"
      ],
      "metadata": {
        "id": "UeGDyVaDpczn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pipeline names\n",
        "nlp.pipe_names"
      ],
      "metadata": {
        "id": "grxmbI2H6H5Q",
        "outputId": "0f183110-3c3b-4c9a-940b-0a2c0a3d0552",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['transformer', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Performing basic NLP tasks using spaCy"
      ],
      "metadata": {
        "id": "cVztM7aSHg-N"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWwX8KlNK3NX"
      },
      "source": [
        "Calling the Language object, `nlp`, on a string of text will return a processed *Doc*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "QLEKXAxxK3NX",
        "outputId": "1f0ec1d4-cc68-4389-bea7-34f5bcad0805",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Apple is looking at buying U.K. startup for $1 billion.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "# An example sentence\n",
        "text = \"Apple is looking at buying U.K. startup for $1 billion.\"\n",
        "text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "beDKSs4XK3NY",
        "outputId": "60ffd2f5-8e99-4a0e-aac3-71cc3d381d37",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/thinc/shims/pytorch.py:114: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(self._mixed_precision):\n"
          ]
        }
      ],
      "source": [
        "# Feed the string object under 'text' to the Language object under 'nlp'\n",
        "# Store the result under the variable 'doc'\n",
        "doc = nlp(text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(doc)"
      ],
      "metadata": {
        "id": "-d28toejq29Y",
        "outputId": "b452edf8-9542-4161-dcaa-8d13c5941ab9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "spacy.tokens.doc.Doc"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xDMhpziK3NX"
      },
      "source": [
        "Passing the variable `text` to the _Language_ object `nlp` returns a spaCy *Doc* object, short for document.\n",
        "\n",
        "This object contains both the input text stored under `text` and the results of natural language processing using spaCy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "69rWTI8oK3NY",
        "outputId": "5a9ee769-cff3-4089-d35b-1ea1de94648b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Apple is looking at buying U.K. startup for $1 billion."
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "# Call the variable to examine the object\n",
        "doc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLU_RXJuK3NZ"
      },
      "source": [
        "Calling the variable `doc` returns the contents of the object.\n",
        "\n",
        "Although the output resembles that of a Python string, the *Doc* object contains a wealth of information about its linguistic structure, which spaCy generated by passing the text through the NLP pipeline.\n",
        "\n",
        "Let's examine the tasks that were performed under the hood after the input sentence was provided to the language model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhcEc6AiK3NZ"
      },
      "source": [
        "### Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybAUeP7dK3Na"
      },
      "source": [
        "*Tokenization* breaks the text down into words, punctuation and so on.\n",
        "\n",
        "The diagram below outlines the tasks that spaCy can perform after a text has been tokenised, such as *part-of-speech tagging*, *syntactic parsing* and *named entity recognition*.\n",
        "\n",
        "<img src='https://cdn.iisc.talentsprint.com/AIandMLOps/Images/spacy_pipeline.png' width=800px>\n",
        "\n",
        "Each *Doc* consists of individual tokens, and we can iterate over them.\n",
        "\n",
        "Let's print out each *Token* object stored in the _Doc_ object `doc`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "OxUEyrg8K3Nb",
        "outputId": "6c9f2f62-6489-4099-c617-124700845984",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token\n",
            "====================\n",
            "Apple\n",
            "is\n",
            "looking\n",
            "at\n",
            "buying\n",
            "U.K.\n",
            "startup\n",
            "for\n",
            "$\n",
            "1\n",
            "billion\n",
            ".\n"
          ]
        }
      ],
      "source": [
        "# Tokens present inside the document\n",
        "\n",
        "# Print the header \"Token\" followed by a separator' line, then iterate through the tokens in the processed text (doc) and print each token's text.\n",
        "print(\"Token\\n\" + '='*20)  # Display \"Token\" as a header with a line of '=' for clarity.\n",
        "\n",
        "for token in doc:  # Loop through each token in the SpaCy Doc object.\n",
        "    print(token.text)  # Print the actual text of each token.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KJaYn-3K3Nc"
      },
      "source": [
        "### Part-of-speech tagging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnGIsQRHK3Nd"
      },
      "source": [
        "Part-of-speech (POS) tagging is the task of determining the word class of a token. This is crucial for *disambiguation*, because different parts of speech may have similar forms.\n",
        "\n",
        ">Consider the example: *The sailor dogs the hatch*.<br>\n",
        ">The present tense of the verb *dog* (to fasten something with something) is precisely the same as the plural form of the noun *dog*: *dogs*.\n",
        "\n",
        "To identify the correct word class, we must examine the context in which the word appears.\n",
        "\n",
        "spaCy provides two types of part-of-speech tags, coarse and fine-grained, which are stored under the attributes `pos_` and `tag_`, respectively.\n",
        "\n",
        "To access the results of POS tagging, let's loop over the *Doc* object `doc` and print each *Token* and its part-of-speech tags."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the token, its coarse-grained (general) POS tag, and fine-grained (detailed) POS tag for each token in the processed text.\n",
        "\n",
        "# Header for the table with proper formatting for alignment\n",
        "print(f\"{' ':<30}POS tag\\n{' ':<20}{'-'*25}\")  # Display a header for \"POS tag\" section with a separator.\n",
        "print(f\"{'Token':<20}{'Coarse':<13}Fine-grained\\n{'='*45}\")  # Define columns: \"Token\", \"Coarse\" (POS), and \"Fine-grained\" tags.\n",
        "\n",
        "for token in doc:  # Iterate over each token in the SpaCy Doc object.\n",
        "    coarse = token.pos_  # Retrieve the coarse-grained POS tag (e.g., NOUN, VERB).\n",
        "    fine = token.tag_  # Retrieve the fine-grained POS tag (e.g., singular noun or past-tense verb).\n",
        "\n",
        "    # Print each token's text, coarse POS tag, and fine-grained POS tag in formatted columns.\n",
        "    print(f\"{token.text:<20}{coarse:<13}{fine}\")\n"
      ],
      "metadata": {
        "id": "Va5tmZcItRvH",
        "outputId": "7d15484b-902f-438b-8200-0bcf1169919f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                              POS tag\n",
            "                    -------------------------\n",
            "Token               Coarse       Fine-grained\n",
            "=============================================\n",
            "Apple               PROPN        NNP\n",
            "is                  AUX          VBZ\n",
            "looking             VERB         VBG\n",
            "at                  ADP          IN\n",
            "buying              VERB         VBG\n",
            "U.K.                PROPN        NNP\n",
            "startup             NOUN         NN\n",
            "for                 ADP          IN\n",
            "$                   SYM          $\n",
            "1                   NUM          CD\n",
            "billion             NUM          CD\n",
            ".                   PUNCT        .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8u4RTfgK3Ny"
      },
      "source": [
        "### Lemmatization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcxwD63HK3Nz"
      },
      "source": [
        "A **lemma** is the base form of a word.\n",
        "\n",
        "Unless explicitly instructed, computers cannot tell the difference between singular and plural forms of words, but treat them as distinct tokens, because their forms differ.\n",
        "\n",
        "For instance, if we want to count the occurrences of words, a process known as _lemmatization_ is needed to group together the different forms of the same token.\n",
        "\n",
        "Lemmas are available for each _Token_ under the attribute `lemma_`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "JtMeYKd4K3Nz",
        "outputId": "deb09984-091c-478f-d56b-925ae4704a5b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token                Lemma\n",
            "==============================\n",
            "Apple                Apple\n",
            "is                   be\n",
            "looking              look\n",
            "at                   at\n",
            "buying               buy\n",
            "U.K.                 U.K.\n",
            "startup              startup\n",
            "for                  for\n",
            "$                    $\n",
            "1                    1\n",
            "billion              billion\n",
            ".                    .\n"
          ]
        }
      ],
      "source": [
        "# Print each token along with its base form (lemma) from the processed text.\n",
        "\n",
        "# Header for the table with proper formatting\n",
        "print(f\"{'Token':<20} Lemma\\n{'='*30}\")  # Define columns: \"Token\" and \"Lemma\" with a separator.\n",
        "\n",
        "for token in doc:  # Iterate over each token in the SpaCy Doc object.\n",
        "    lemma = token.lemma_  # Retrieve the lemma (base form) of the token.\n",
        "\n",
        "    # Print the token's text and its corresponding lemma in formatted columns.\n",
        "    print(f\"{token.text:<20} {lemma}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Removing punctuations, stop words, converting to lowercase"
      ],
      "metadata": {
        "id": "qnstEWPGR_5U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Access and print the stop words list provided by SpaCy for English.\n",
        "\n",
        "# Retrieve SpaCy's predefined set of English stop words.\n",
        "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
        "\n",
        "# Print the total number of stop words in the set.\n",
        "print(len(spacy_stopwords))\n",
        "\n",
        "# Print the complete list of stop words.\n",
        "print(spacy_stopwords)\n"
      ],
      "metadata": {
        "id": "jPZjVnY-SUET",
        "outputId": "3010c40a-011d-4a49-f1a5-e52f6eeadfb5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "326\n",
            "{'alone', 'some', 'beside', 'perhaps', '’ve', 'whence', 'seem', 'go', 'however', 'almost', 'cannot', 'various', 'never', 'please', 'every', 'when', \"'ll\", 'hence', 'thereafter', 'also', 'since', 'nor', 'now', 'its', 'as', 'anywhere', 'often', 'therefore', 'they', 'herein', 'someone', 'amount', 'always', 'after', 'forty', 'off', 'did', 'this', 'yours', 'have', 'somewhere', 'due', 'noone', 'where', 'just', 'next', 'otherwise', 'several', 'herself', 'still', 'rather', 'three', 'wherein', 'behind', 'two', 'below', 'hundred', 'too', 'whoever', 'without', 'latterly', 'me', 'besides', 'hereupon', 'indeed', 'towards', 'himself', 'part', 'if', 'show', 'whereupon', 'n’t', 'by', 'namely', 'across', 'themselves', 'call', 'seems', 'can', 'wherever', 'afterwards', 'my', 'become', 'first', 'during', 'via', 'but', 'except', 'thereby', 'down', 'keep', 'made', 'none', 'yourselves', 'another', 'bottom', 'sixty', 'unless', 'whereby', 'one', 'fifteen', 'most', 'even', 'through', 'something', 'being', 'twenty', 'are', 'their', 'nevertheless', 'were', 'five', 'twelve', 'once', 'only', 'say', 'yourself', 'or', 'thence', 'same', 'who', 'while', 'six', 'latter', \"'s\", 'beforehand', 'fifty', 'everyone', 'about', 'i', 'should', \"'d\", 'any', 'seemed', 'ten', '’m', 'at', 'he', '‘s', 'side', 'was', 'less', 'mine', 'therein', 'it', 'until', 'eight', 'has', 'does', 'no', 'everywhere', 'nine', 'upon', 'sometime', 'empty', 'both', 'further', 'own', 'see', 'of', 'above', 'each', 'to', 'ca', 'well', 'many', 'full', 'thus', 'your', 'neither', 'thru', 'we', 'under', 'nobody', 'quite', 'others', 'four', 'somehow', 'used', 'so', 'be', 'not', 'would', 'more', '’d', 'whereas', 'using', 'thereupon', 'either', 'what', 'nothing', 'least', 'do', 'already', 'and', 'a', 'there', \"'ve\", 'whose', 'move', 'here', 'among', '‘ll', 'them', 'ever', 'though', 'over', 'amongst', 'few', 'very', 'the', 'up', 'becomes', 'you', 'with', '’ll', 'such', 'anyone', 'other', '‘m', 'could', 'had', 'than', 'whereafter', 'whole', 'between', 'back', 'his', 'all', 'into', \"n't\", \"'re\", 'together', 'elsewhere', 'an', 'became', 'beyond', 'will', 'really', 'might', 'that', 'around', 'anyway', 'front', 'serious', 'from', 'much', '‘ve', 'done', 'her', 'take', 'third', 'becoming', 'formerly', 'mostly', 'get', 'former', 'last', 'which', 'along', 'within', 'meanwhile', 'else', 'n‘t', 'hers', 'against', 'enough', 'hereby', 'then', 'must', 'throughout', 'top', 'whenever', 'nowhere', 'put', 'why', 'doing', 'because', 'whatever', 'how', '‘re', 'although', 'anything', 'whether', 'name', 'in', 'regarding', 'ourselves', 'on', 'myself', 'sometimes', 'ours', 'again', \"'m\", 'these', 'toward', 'onto', '‘d', 'been', 'am', 'she', 'him', 'eleven', 'our', 'yet', 'before', 'for', 'moreover', 'may', 're', 'per', 'out', 'is', '’re', 'anyhow', 'itself', 'give', 'those', '’s', 'whom', 'us', 'make', 'everything', 'hereafter', 'whither', 'seeming'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check and print whether each token is a stop word or a punctuation mark.\n",
        "\n",
        "# Header for the table with proper formatting\n",
        "print(f\"{'Token':<20} {'Is stopword':<15} Is Punctuation\\n{'='*55}\")\n",
        "# Define columns: \"Token\", \"Is stopword\", and \"Is Punctuation\" with a separator line.\n",
        "\n",
        "for token in doc:  # Iterate through each token in the SpaCy Doc object.\n",
        "    stop = token.is_stop  # Check if the token is a stop word (returns True/False).\n",
        "    punct = token.is_punct  # Check if the token is a punctuation mark (returns True/False).\n",
        "\n",
        "    # Print the token's text, whether it's a stop word, and whether it's a punctuation mark.\n",
        "    print(f\"{token.text:<20} {stop:<15} {punct}\")\n"
      ],
      "metadata": {
        "id": "PntYPVjkVOfb",
        "outputId": "d0872716-66bd-4f83-c9d1-15dc8ad6aa9a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token                Is stopword     Is Punctuation\n",
            "=======================================================\n",
            "Apple                0               False\n",
            "is                   1               False\n",
            "looking              0               False\n",
            "at                   1               False\n",
            "buying               0               False\n",
            "U.K.                 0               False\n",
            "startup              0               False\n",
            "for                  1               False\n",
            "$                    0               False\n",
            "1                    0               False\n",
            "billion              0               False\n",
            ".                    0               True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_token(token):\n",
        "\n",
        "    \"\"\" If a token is not a stop word and not a punctuation,\n",
        "        then reduce it to its base form, remove trailing spaces, and covert to lowercase. \"\"\"\n",
        "\n",
        "    if not token.is_stop and not token.is_punct:  # Check if the token is neither a stop word nor punctuation.\n",
        "        return token.lemma_.strip().lower()  # Return the lemmatized, trimmed, and lowercase version of the token."
      ],
      "metadata": {
        "id": "11yH1FR4XAdq"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess each token in the text and print both the original token and the preprocessed version.\n",
        "\n",
        "# Header for the output table with proper formatting\n",
        "print(f\"{'Token':<20} Preprocessed token\\n{'='*40}\")\n",
        "# Define columns: \"Token\" (original text) and \"Preprocessed token\" with a separator line.\n",
        "\n",
        "for token in doc:  # Iterate through each token in the SpaCy Doc object.\n",
        "    output = preprocess_token(token)  # Call the preprocess_token function to process the token.\n",
        "\n",
        "    # Print the original token text and its preprocessed version.\n",
        "    # If the token is a stop word or punctuation, the preprocessed version will be None.\n",
        "    print(f\"{token.text:<20} {output}\")\n"
      ],
      "metadata": {
        "id": "ZoCOU8gSXz_h",
        "outputId": "06f6be12-6033-4fb4-ea85-97b482ad1743",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token                Preprocessed token\n",
            "========================================\n",
            "Apple                apple\n",
            "is                   None\n",
            "looking              look\n",
            "at                   None\n",
            "buying               buy\n",
            "U.K.                 u.k.\n",
            "startup              startup\n",
            "for                  None\n",
            "$                    $\n",
            "1                    1\n",
            "billion              billion\n",
            ".                    None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyeCdI6iK3Nz"
      },
      "source": [
        "### Named Entity Recognition (NER)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mv2U9VEzK3N0"
      },
      "source": [
        "Named entity recognition (NER) is the task of recognising and classifying entities named in a text.\n",
        "\n",
        "spaCy can recognise the named entities such as persons, geographic locations, and products as these were annotated in the dataset its trained on (OntoNotes 5 corpus).\n",
        "\n",
        "We can use the *Doc* object's `.ents` attribute to get the named entities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "9wn523SiK3N0",
        "outputId": "8336d3f8-fdd1-4bed-d7c4-3ecdf2be123a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Apple, U.K., $1 billion)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "# Access and display the named entities in the processed text (doc).\n",
        "doc.ents  # This returns a list of all named entities recognized by SpaCy in the processed text."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This returns a tuple with the named entities.\n",
        "\n",
        "Each item in the tuple is a spaCy *Span* object. *Span* objects can consist of multiple *Token* objects, as many named entities span multiple *Tokens*."
      ],
      "metadata": {
        "id": "oM4MeyX2qlas"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the type of the object that stores the named entities\n",
        "type(doc.ents[0])  # This will return the type of the first entity in the list of named entities."
      ],
      "metadata": {
        "id": "NGs7oHNjqMGu",
        "outputId": "eb6daa95-7a6d-40ec-c304-3a20f8ab6a18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "spacy.tokens.span.Span"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yaCzY90CK3N0"
      },
      "source": [
        "The named entities and their types are stored under the attributes `.text` and `.label_` of each *Span* object.\n",
        "\n",
        "Let's loop over the *Span* objects in the tuple and print out both attributes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "yveeIWxXK3N0",
        "outputId": "d8d1d83b-cf86-4a51-ee7a-e26d658e37ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text                 Entity_label     Explanation\n",
            "================================================================================\n",
            "Apple                ORG              Companies, agencies, institutions, etc.\n",
            "U.K.                 GPE              Countries, cities, states\n",
            "$1 billion           MONEY            Monetary values, including unit\n"
          ]
        }
      ],
      "source": [
        "# Loop over the named entities in the Doc object, and print each entity with its label and explanation.\n",
        "\n",
        "# Header for the output table with proper formatting\n",
        "print(f\"{'Text':<20} {'Entity_label':<16} Explanation\\n{'='*80}\")\n",
        "# Define columns: \"Text\" (named entity), \"Entity_label\" (entity type), and \"Explanation\" (label description).\n",
        "\n",
        "for ent in doc.ents:  # Iterate over each named entity in the doc.ents list.\n",
        "    ent_text = ent.text           # Get the text of the named entity (e.g., 'Apple Inc.')\n",
        "    ent_label = ent.label_        # Get the entity label (e.g., 'ORG' for organization)\n",
        "    ent_label_val = spacy.explain(ent_label)  # Get the explanation of the entity label (e.g., 'Organization')\n",
        "\n",
        "    # Print the entity text, its label, and the explanation of the label.\n",
        "    print(f\"{ent_text:<20} {ent_label:<16} {ent_label_val}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GCgHkJ7K3N1"
      },
      "source": [
        "As you can see, named entities like '$1 billion' identified in the *Doc* consist of multiple *Tokens*, which is why they are represented as *Span* objects."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGjkG8m3K3N2"
      },
      "source": [
        "spaCy [*Span*](https://spacy.io/api/span) objects contain several useful arguments.\n",
        "\n",
        "Most importantly, the attributes `start` and `end` return the indices of _Tokens_, which determine where the _Span_ starts and ends in the *Doc* object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "kPwNfX-VK3N2",
        "outputId": "a5410b73-43be-4d9d-95b9-8edc2d857178",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "$1 billion 8 11\n"
          ]
        }
      ],
      "source": [
        "# Print the named entity and its start and end token indices in the Doc.\n",
        "print(doc.ents[2], doc.ents[2].start, doc.ents[2].end)\n",
        "# doc.ents[2] accesses the third named entity in the Doc (indexing starts at 0).\n",
        "# doc.ents[2].start gives the index of the first token of the named entity.\n",
        "# doc.ents[2].end gives the index of the token just after the last token of the named entity.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z29gu0lwK3N3"
      },
      "source": [
        "The named entity starts at index 8 and ends at index 11 in the *Doc* object."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Visualize Named Entities"
      ],
      "metadata": {
        "id": "3ycEVv9KrNkZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTKE_TnYK3N4"
      },
      "source": [
        "We can also render the named entities using *displacy*, the spaCy module we used for visualising dependency parses above.\n",
        "\n",
        "Note that we must pass the string `ent` to the `style` argument to indicate that we wish to visualise named entities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "lUN9_a2KK3N5",
        "outputId": "139172d9-1a39-4b5d-815b-66b223e1c049",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Apple\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " is looking at buying \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    U.K.\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              " startup for \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    $1 billion\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
              "</mark>\n",
              ".</div></span>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Visualize the named entities in the document using SpaCy's displacy visualizer.\n",
        "spacy.displacy.render(doc, style='ent')\n",
        "# `style='ent'` specifies that we want to visualize the named entities in the document.\n",
        "# This will highlight and display the named entities with their labels (e.g., ORG, PERSON, GPE) in the text."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Test another example 2:**"
      ],
      "metadata": {
        "id": "I5fLOBFpxSt1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize named entities in a new sample text using SpaCy's displacy visualizer.\n",
        "\n",
        "text2 = \"On 3rd Feb, Ram was in Delhi.\\nLater he traveled to Mumbai via Air India flight reading a Time magazine to meet Raj.\\nAfter 10 days, he went again back to Delhi wearing a Timex watch.\"\n",
        "# This is a new sample text to analyze.\n",
        "\n",
        "doc2 = nlp(text2)  # Process the new text using the SpaCy NLP pipeline (the `nlp` object).\n",
        "spacy.displacy.render(doc2, style='ent')\n",
        "# Render the named entities in the new `doc2`, highlighting and labeling them in the text."
      ],
      "metadata": {
        "id": "kZLngerFvbL7",
        "outputId": "8152c33d-6839-4a49-fc94-7c6ec8ed3e43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">On \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    3rd Feb\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              ", \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Ram\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " was in \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Delhi\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              ".<br>Later he traveled to \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Mumbai\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              " via \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Air India\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " flight reading a \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Time\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " magazine to meet \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Raj\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              ".<br>After \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    10 days\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              ", he went again back to \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Delhi\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              " wearing a \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Timex\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " watch.</div></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If a particular tag used for a named entity is unfamiliar, you can check it's explanation."
      ],
      "metadata": {
        "id": "ogdrSb382R3J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the explanation for the 'DATE' entity label using SpaCy's explain function.\n",
        "spacy.explain('DATE')\n",
        "# This will return a human-readable explanation for the entity label 'DATE'."
      ],
      "metadata": {
        "id": "D-dFic41xMda",
        "outputId": "7be9d4a5-5f33-498e-c442-fc019487b622",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Absolute or relative dates or periods'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the explanation for the 'PERSON' entity label using SpaCy's explain function.\n",
        "spacy.explain('PERSON')\n",
        "# This will return a human-readable explanation for the entity label 'PERSON'."
      ],
      "metadata": {
        "id": "IBp1zyMXxnJ6",
        "outputId": "1a950bf5-2b8d-4450-ca4a-23550e786ac8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'People, including fictional'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Test another example 3:**"
      ],
      "metadata": {
        "id": "XAvWA3882Ues"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize named entities in another sample text using SpaCy's displacy visualizer.\n",
        "\n",
        "text3 = \"Holmes solves his another case while sitting at his home in Baker Street, without moving a single inch.\"\n",
        "# This is another sample text to analyze, mentioning a character and a location.\n",
        "\n",
        "doc3 = nlp(text3)  # Process the new text using the SpaCy NLP pipeline (the `nlp` object).\n",
        "spacy.displacy.render(doc3, style='ent')\n",
        "# Render the named entities in the new `doc3`, highlighting and labeling them in the text.\n"
      ],
      "metadata": {
        "id": "UV6e_njh1LKX",
        "outputId": "4e573ced-7542-41e7-a86f-db25dea1a65f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Holmes\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " solves his another case while sitting at his home in \n",
              "<mark class=\"entity\" style=\"background: #9cc9cc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Baker Street\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">FAC</span>\n",
              "</mark>\n",
              ", without moving \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    a single inch\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">QUANTITY</span>\n",
              "</mark>\n",
              ".</div></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the explanation for the 'FAC' entity label using SpaCy's explain function.\n",
        "spacy.explain('FAC')\n",
        "# This will return a human-readable explanation for the entity label 'FAC'."
      ],
      "metadata": {
        "id": "L76wtVpb1n6a",
        "outputId": "d8f659a4-45ee-4455-ead6-b879ce3f53ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Buildings, airports, highways, bridges, etc.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "References:\n",
        "* https://spacy.io/usage/spacy-101"
      ],
      "metadata": {
        "id": "YFXGM68ARzhb"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHfHdGCP_n6Y"
      },
      "source": [
        "### Please answer the questions below to complete the experiment:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VgSwVENIPcM6"
      },
      "outputs": [],
      "source": [
        "#@title Which of the following is a technique to convert a word into its base form? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Answer = \"\" #@param [\"\", \"Tokenization\", \"Part-of-Speech Tagging\", \"Lemmatization\", \"Named Entity Recognition\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMzKSbLIgFzQ"
      },
      "outputs": [],
      "source": [
        "#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Complexity = \"\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DjcH1VWSFI2l"
      },
      "outputs": [],
      "source": [
        "#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n",
        "Additional = \"\" #@param {type:\"string\"}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4VBk_4VTAxCM"
      },
      "outputs": [],
      "source": [
        "#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Concepts = \"\" #@param [\"\",\"Yes\", \"No\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XH91cL1JWH7m"
      },
      "outputs": [],
      "source": [
        "#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Comments = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8xLqj7VWIKW"
      },
      "outputs": [],
      "source": [
        "#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Mentor_support = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "FzAZHt1zw-Y-"
      },
      "outputs": [],
      "source": [
        "#@title Run this cell to submit your notebook for grading { vertical-output: true }\n",
        "try:\n",
        "  if submission_id:\n",
        "      return_id = submit_notebook()\n",
        "      if return_id : submission_id = return_id\n",
        "  else:\n",
        "      print(\"Please complete the setup first.\")\n",
        "except NameError:\n",
        "  print (\"Please complete the setup first.\")"
      ]
    }
  ]
}